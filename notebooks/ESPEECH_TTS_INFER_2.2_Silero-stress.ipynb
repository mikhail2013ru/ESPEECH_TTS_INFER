{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F6YzVP_1AlhR",
        "outputId": "50a3b64e-634e-477f-b669-c7a6454cbfb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting f5-tts\n",
            "  Downloading f5_tts-1.1.7-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.42.0)\n",
            "Collecting ruaccent\n",
            "  Downloading ruaccent-1.5.8.3-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: accelerate>=0.33.0 in /usr/local/lib/python3.12/dist-packages (from f5-tts) (1.10.0)\n",
            "Collecting bitsandbytes>0.37.0 (from f5-tts)\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Collecting cached_path (from f5-tts)\n",
            "  Downloading cached_path-1.7.3-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from f5-tts) (8.2.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from f5-tts) (4.0.0)\n",
            "Collecting ema_pytorch>=0.5.2 (from f5-tts)\n",
            "  Downloading ema_pytorch-0.7.7-py3-none-any.whl.metadata (689 bytes)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.35.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting hydra-core>=1.3.0 (from f5-tts)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (from f5-tts) (0.42.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from f5-tts) (0.11.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from f5-tts) (3.10.0)\n",
            "Collecting numpy<=1.26.4 (from f5-tts)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<=2.10.6 (from f5-tts)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from f5-tts) (0.25.1)\n",
            "Collecting pypinyin (from f5-tts)\n",
            "  Downloading pypinyin-0.55.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from f5-tts) (0.6.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from f5-tts) (0.13.1)\n",
            "Collecting tomli (from f5-tts)\n",
            "  Downloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchdiffeq (from f5-tts)\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from f5-tts) (4.67.1)\n",
            "Collecting transformers_stream_generator (from f5-tts)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode (from f5-tts)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting vocos (from f5-tts)\n",
            "  Downloading vocos-0.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from f5-tts) (0.21.1)\n",
            "Collecting x_transformers>=1.31.14 (from f5-tts)\n",
            "  Downloading x_transformers-2.7.2-py3-none-any.whl.metadata (90 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Collecting gradio-client==1.10.4 (from gradio)\n",
            "  Downloading gradio_client-1.10.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.10.4->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
            "Collecting onnxruntime (from ruaccent)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from ruaccent) (0.2.1)\n",
            "Collecting python-crfsuite (from ruaccent)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting razdel (from ruaccent)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.33.0->f5-tts) (5.9.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.0->f5-tts) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.0->f5-tts) (4.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.10.6->f5-tts) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->f5-tts)\n",
            "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from x_transformers>=1.31.14->f5-tts) (0.8.1)\n",
            "Collecting einx>=0.3.0 (from x_transformers>=1.31.14->f5-tts)\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting loguru (from x_transformers>=1.31.14->f5-tts)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting boto3<2.0,>=1.0 (from cached_path->f5-tts)\n",
            "  Downloading boto3-1.40.17-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from cached_path->f5-tts) (2.19.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->f5-tts) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->f5-tts) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->f5-tts) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->f5-tts) (0.70.16)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5-tts) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->f5-tts) (1.17.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5-tts) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5-tts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5-tts) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5-tts) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5-tts) (3.2.3)\n",
            "Collecting coloredlogs (from onnxruntime->ruaccent)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->ruaccent) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->ruaccent) (5.29.5)\n",
            "Collecting encodec==0.1.1 (from vocos->f5-tts)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->f5-tts) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->f5-tts) (4.3.8)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->f5-tts) (2.35.0)\n",
            "Collecting botocore<1.41.0,>=1.40.17 (from boto3<2.0,>=1.0->cached_path->f5-tts)\n",
            "  Downloading botocore-1.40.17-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached_path->f5-tts)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3<2.0,>=1.0->cached_path->f5-tts)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->f5-tts) (2.22)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from einx>=0.3.0->x_transformers>=1.31.14->f5-tts) (2.4.6)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (3.12.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->f5-tts) (4.0.12)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (1.7.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->f5-tts) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->f5-tts) (3.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->ruaccent)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5-tts) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->f5-tts) (5.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (4.9.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5-tts) (0.6.1)\n",
            "Downloading f5_tts-1.1.7-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.35.0-py3-none-any.whl (54.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.3/54.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.4-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruaccent-1.5.8.3-py2.py3-none-any.whl (22 kB)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ema_pytorch-0.7.7-py3-none-any.whl (9.8 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading x_transformers-2.7.2-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cached_path-1.7.3-py3-none-any.whl (36 kB)\n",
            "Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin-0.55.0-py2.py3-none-any.whl (840 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vocos-0.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading boto3-1.40.17-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.17-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers_stream_generator, encodec\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12426 sha256=333a252b5e108a2539bc509609afcfb5b28fc7406f8f79eb495f0d0172626bc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/58/d2/014cb67c3cc6def738c1b1635dbf4e3dab6fb63aba7070dce0\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=034091451cb5c73fb9a0d09bcc151bffeb20f0b03a588a4968af8cf63620d156\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/eb/9f/e13610cc46ab39d3199fbabebd1c3e142d44b679526e0f228a\n",
            "Successfully built transformers_stream_generator encodec\n",
            "Installing collected packages: razdel, unidecode, tomli, python-crfsuite, pypinyin, pydantic-core, numpy, loguru, jmespath, humanfriendly, pydantic, hydra-core, einx, coloredlogs, botocore, s3transfer, onnxruntime, gradio-client, x_transformers, torchdiffeq, gradio, ema_pytorch, boto3, bitsandbytes, transformers_stream_generator, ruaccent, encodec, vocos, cached_path, f5-tts\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.11.1\n",
            "    Uninstalling gradio_client-1.11.1:\n",
            "      Successfully uninstalled gradio_client-1.11.1\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.42.0\n",
            "    Uninstalling gradio-5.42.0:\n",
            "      Successfully uninstalled gradio-5.42.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mcp 1.13.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.47.0 boto3-1.40.17 botocore-1.40.17 cached_path-1.7.3 coloredlogs-15.0.1 einx-0.3.0 ema_pytorch-0.7.7 encodec-0.1.1 f5-tts-1.1.7 gradio-5.35.0 gradio-client-1.10.4 humanfriendly-10.0 hydra-core-1.3.2 jmespath-1.0.1 loguru-0.7.3 numpy-1.26.4 onnxruntime-1.22.1 pydantic-2.10.6 pydantic-core-2.27.2 pypinyin-0.55.0 python-crfsuite-0.9.11 razdel-0.5.0 ruaccent-1.5.8.3 s3transfer-0.13.1 tomli-2.2.1 torchdiffeq-0.2.5 transformers_stream_generator-0.0.5 unidecode-1.4.0 vocos-0.1.0 x_transformers-2.7.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "380b02460b5e481784b8f0097d255744",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install gradio runorm transformers torch torchaudio huggingface_hub soundfile pydub silero-stress git+https://github.com/mikhail2013ru/F5-TTS-RUS.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXMWDSiTA_Gf"
      },
      "outputs": [],
      "source": [
        "# ĞĞĞ–ĞĞ¢Ğ¬ RESTART SESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i90o6-p9AvOA",
        "outputId": "f9458d9f-4cad-4393-b9f2-b4b4f08af84d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking CUDA availability...\n",
            "CUDA is available. Using device: Tesla T4\n",
            "Preloading model...\n",
            "CUDA is available. Using device: Tesla T4\n",
            "Trying to download model file 'espeech_tts_rlv2.pt' and 'vocab.txt' from hub 'ESpeech/ESpeech-TTS-1_RL-V2'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded model to /root/.cache/huggingface/hub/models--ESpeech--ESpeech-TTS-1_RL-V2/snapshots/f582b6e5897fe8a5835059405a8439d13bdf7684/espeech_tts_rlv2.pt\n",
            "Downloaded vocab to /root/.cache/huggingface/hub/models--ESpeech--ESpeech-TTS-1_RL-V2/snapshots/f582b6e5897fe8a5835059405a8439d13bdf7684/vocab.txt\n",
            "Loading model from: /root/.cache/huggingface/hub/models--ESpeech--ESpeech-TTS-1_RL-V2/snapshots/f582b6e5897fe8a5835059405a8439d13bdf7684/espeech_tts_rlv2.pt\n",
            "\n",
            "vocab :  /root/.cache/huggingface/hub/models--ESpeech--ESpeech-TTS-1_RL-V2/snapshots/f582b6e5897fe8a5835059405a8439d13bdf7684/vocab.txt\n",
            "token :  custom\n",
            "model :  /root/.cache/huggingface/hub/models--ESpeech--ESpeech-TTS-1_RL-V2/snapshots/f582b6e5897fe8a5835059405a8439d13bdf7684/espeech_tts_rlv2.pt \n",
            "\n",
            "Model loaded and moved to CUDA: cuda\n",
            "Model preloaded.\n",
            "Loading RUAccent...\n",
            "RUAccent loaded.\n",
            "Preloading vocoder...\n",
            "CUDA is available. Using device: Tesla T4\n",
            "Loading vocoder...\n",
            "Download Vocos from huggingface charactr/vocos-mel-24khz\n",
            "Vocoder loaded and moved to CUDA: cuda\n",
            "Vocoder preloaded.\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://29f0abcf0bb8391bb1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://29f0abcf0bb8391bb1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import gc\n",
        "import tempfile\n",
        "import traceback\n",
        "import json\n",
        "import threading\n",
        "from pydub import AudioSegment\n",
        "from pathlib import Path\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "from huggingface_hub import login, hf_hub_download, snapshot_download\n",
        "\n",
        "import gradio.blocks\n",
        "import gradio.components\n",
        "\n",
        "# ================ Ğ£Ğ›Ğ£Ğ§Ğ¨Ğ•ĞĞĞĞ¯ ĞĞ’Ğ¢ĞĞ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ HUGGINGFACE ================\n",
        "def setup_huggingface_auth():\n",
        "    \"\"\"ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ HuggingFace Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ÑĞ¼\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ” ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ ĞĞ’Ğ¢ĞĞ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ HUGGINGFACE\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°\n",
        "    hf_token = None\n",
        "    token_source = \"ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½\"\n",
        "    \n",
        "    # 1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ\n",
        "    if os.environ.get(\"HF_TOKEN\"):\n",
        "        hf_token = os.environ[\"HF_TOKEN\"]\n",
        "        token_source = \"Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ HF_TOKEN\"\n",
        "    \n",
        "    # 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼\n",
        "    elif os.path.exists(\"/root/.cache/huggingface/token\"):\n",
        "        try:\n",
        "            with open(\"/root/.cache/huggingface/token\", \"r\") as f:\n",
        "                hf_token = f.read().strip()\n",
        "            if hf_token:\n",
        "                token_source = \"Ñ„Ğ°Ğ¹Ğ» /root/.cache/huggingface/token\"\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°: {e}\")\n",
        "    \n",
        "    # 3. Ğ•ÑĞ»Ğ¸ Ğ² Colab, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞµĞºÑ€ĞµÑ‚Ñ‹\n",
        "    else:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            hf_token = userdata.get('HF_TOKEN')\n",
        "            if hf_token:\n",
        "                token_source = \"ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Colab\"\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸ ĞĞµ Ğ² Colab Ğ¸Ğ»Ğ¸ google.colab Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸Ğ· Colab: {e}\")\n",
        "    \n",
        "    # 4. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ°\n",
        "    if not hf_token:\n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» Hugging Face\n",
        "        hf_cache_token = os.path.expanduser(\"~/.cache/huggingface/token\")\n",
        "        if os.path.exists(hf_cache_token):\n",
        "            try:\n",
        "                with open(hf_cache_token, \"r\") as f:\n",
        "                    hf_token = f.read().strip()\n",
        "                if hf_token:\n",
        "                    token_source = f\"Ñ„Ğ°Ğ¹Ğ» {hf_cache_token}\"\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    # Ğ•ÑĞ»Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒĞµĞ¼ÑÑ\n",
        "    if hf_token:\n",
        "        try:\n",
        "            print(f\"âœ… Ğ¢Ğ¾ĞºĞµĞ½ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² {token_source}\")\n",
        "            \n",
        "            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ°\n",
        "            if not hf_token.startswith(\"hf_\"):\n",
        "                print(f\"âš ï¸ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¢Ğ¾ĞºĞµĞ½ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°. Ğ”Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒÑÑ Ñ 'hf_'\")\n",
        "            \n",
        "            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ\n",
        "            login(token=hf_token)\n",
        "            \n",
        "            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½ Ğ² Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ\n",
        "            os.environ[\"HF_TOKEN\"] = hf_token\n",
        "            \n",
        "            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½ Ğ² Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞµÑÑĞ¸Ğ¹\n",
        "            try:\n",
        "                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ĞµÑĞ»Ğ¸ ĞµÑ‘ Ğ½ĞµÑ‚\n",
        "                os.makedirs(\"/root/.cache/huggingface\", exist_ok=True)\n",
        "                with open(\"/root/.cache/huggingface/token\", \"w\") as f:\n",
        "                    f.write(hf_token)\n",
        "                print(\"âœ… Ğ¢Ğ¾ĞºĞµĞ½ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ²\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½ Ğ² Ñ„Ğ°Ğ¹Ğ»: {e}\")\n",
        "            \n",
        "            print(\"âœ… ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ HuggingFace ÑƒÑĞ¿ĞµÑˆĞ½Ğ°!\")\n",
        "            return hf_token\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: {error_msg}\")\n",
        "            \n",
        "            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸\n",
        "            if \"401\" in error_msg:\n",
        "                print(\"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°.\")\n",
        "            elif \"400\" in error_msg:\n",
        "                print(\"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ¢Ğ¾ĞºĞµĞ½ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒÑÑ Ñ 'hf_'.\")\n",
        "            elif \"connection\" in error_msg.lower():\n",
        "                print(\"âŒ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ñƒ. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ.\")\n",
        "            \n",
        "            return None\n",
        "    else:\n",
        "        print(\"â„¹ï¸  Ğ¢Ğ¾ĞºĞµĞ½ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ±ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.\")\n",
        "        print(\"   Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹.\")\n",
        "        \n",
        "        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"ğŸ“‹ ĞšĞ°Ğº Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½:\")\n",
        "        print(\"1. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½ Ğ½Ğ° https://huggingface.co/settings/tokens\")\n",
        "        print(\"2. Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸Ğ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ²:\")\n",
        "        print(\"   â€¢ Ğ’ Colab: Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ² ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ñ ĞºĞ»ÑÑ‡Ğ¾Ğ¼ 'HF_TOKEN'\")\n",
        "        print(\"   â€¢ Ğ’ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ: export HF_TOKEN='Ğ²Ğ°Ñˆ_Ñ‚Ğ¾ĞºĞµĞ½'\")\n",
        "        print(\"   â€¢ Ğ’ Ñ„Ğ°Ğ¹Ğ»: echo 'Ğ²Ğ°Ñˆ_Ñ‚Ğ¾ĞºĞµĞ½' > /root/.cache/huggingface/token\")\n",
        "        print(\"-\"*60)\n",
        "        \n",
        "        return None\n",
        "\n",
        "# Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸš€ Ğ—ĞĞŸĞ£Ğ¡Ğš ESpeech-TTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "HF_TOKEN = setup_huggingface_auth()\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================ Ğ“Ğ›ĞĞ‘ĞĞ›Ğ¬ĞĞ«Ğ• Ğ¤Ğ›ĞĞ“Ğ˜ Ğ”Ğ›Ğ¯ Ğ£ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ¯ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ•Ğ™ ================\n",
        "stop_generation = False\n",
        "generation_thread = None\n",
        "generation_event = threading.Event()\n",
        "stop_requested = False\n",
        "\n",
        "from silero_stress import load_accentor\n",
        "from runorm import RUNorm\n",
        "from f5_tts.infer.utils_infer import (\n",
        "    infer_process,\n",
        "    load_model,\n",
        "    load_vocoder,\n",
        "    preprocess_ref_audio_text,\n",
        "    remove_silence_for_generated_wav,\n",
        "    save_spectrogram,\n",
        "    tempfile_kwargs,\n",
        ")\n",
        "from f5_tts.model import DiT\n",
        "\n",
        "# ğŸ”¹ Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ÑÑ‚Ğ¸ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ Whisper\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    \"ESpeech-TTS-1_SFT-95K\": {\n",
        "        \"repo\": \"ESpeech/ESpeech-TTS-1_SFT-95K\",\n",
        "        \"model_file\": \"espeech_tts_95k.pt\",\n",
        "        \"vocab_file\": \"vocab.txt\",\n",
        "        \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "    },\n",
        "    \"ESpeech-TTS-1_RL-V1\": {\n",
        "        \"repo\": \"ESpeech/ESpeech-TTS-1_RL-V1\",\n",
        "        \"model_file\": \"espeech_tts_rlv1.pt\",\n",
        "        \"vocab_file\": \"vocab.txt\",\n",
        "        \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "    },\n",
        "    \"ESpeech-TTS-1_RL-V2\": {\n",
        "        \"repo\": \"ESpeech/ESpeech-TTS-1_RL-V2\",\n",
        "        \"model_file\": \"espeech_tts_rlv2.pt\",\n",
        "        \"vocab_file\": \"vocab.txt\",\n",
        "        \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "    },\n",
        "    \"F5TTS_v1_Base_v2_Misha24-10\": {\n",
        "        \"repo\": \"Misha24-10/F5-TTS_RUSSIAN\",\n",
        "        \"model_file\": \"F5TTS_v1_Base_v2/model_last_inference.safetensors\",\n",
        "        \"vocab_file\": \"F5TTS_v1_Base/vocab.txt\",\n",
        "        \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "    },\n",
        "    \"F5TTS_v1_Base_v4_winter\": {\n",
        "        \"repo\": \"Misha24-10/F5-TTS_RUSSIAN\",\n",
        "        \"model_file\": \"F5TTS_v1_Base_v4_winter/model_212000.safetensors\",\n",
        "        \"vocab_file\": \"F5TTS_v1_Base/vocab.txt\",\n",
        "        \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "    },\n",
        "    \"F5TTS_v1_Base_1.25M_SWivid\": {\n",
        "        \"repo\": \"SWivid/F5-TTS\",\n",
        "        \"model_file\": \"F5TTS_v1_Base/model_1250000.safetensors\",\n",
        "        \"vocab_file\": \"F5TTS_v1_Base/vocab.txt\",\n",
        "        \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "    },\n",
        "}\n",
        "\n",
        "# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "loaded_models = {}\n",
        "loaded_vocoder = None\n",
        "current_model_name = None\n",
        "remember_seed = False\n",
        "last_seed = -1\n",
        "\n",
        "# ğŸ”´ ĞĞĞ’ĞĞ•: Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "local_models_config = {}\n",
        "\n",
        "# ğŸ”´ ĞĞĞ’ĞĞ•: Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ\n",
        "LOCAL_MODELS_FILE = \"local_models.json\"\n",
        "local_models_config = {}\n",
        "\n",
        "def load_local_models():\n",
        "    \"\"\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°\"\"\"\n",
        "    global local_models_config\n",
        "    try:\n",
        "        if os.path.exists(LOCAL_MODELS_FILE):\n",
        "            with open(LOCAL_MODELS_FILE, 'r', encoding='utf-8') as f:\n",
        "                local_models_config = json.load(f)\n",
        "            print(f\"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(local_models_config)} Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\")\n",
        "        else:\n",
        "            local_models_config = {}\n",
        "            print(\"âœ… Ğ¤Ğ°Ğ¹Ğ» Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: {e}\")\n",
        "        local_models_config = {}\n",
        "\n",
        "def save_local_models():\n",
        "    \"\"\"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ°Ğ¹Ğ»\"\"\"\n",
        "    try:\n",
        "        with open(LOCAL_MODELS_FILE, 'w', encoding='utf-8') as f:\n",
        "            json.dump(local_models_config, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(local_models_config)} Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: {e}\")\n",
        "\n",
        "# ğŸ”´ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ\n",
        "load_local_models()\n",
        "\n",
        "def get_all_models_config():\n",
        "    \"\"\"ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\"\"\"\n",
        "    return {**MODELS_CONFIG, **local_models_config}\n",
        "\n",
        "def clear_loaded_models():\n",
        "    \"\"\"ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ˜ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ´Ñ€Ğ¾Ğ¿Ğ±Ğ¾ĞºÑĞ°\"\"\"\n",
        "    global loaded_models, loaded_vocoder, current_model_name, local_models_config\n",
        "    \n",
        "    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ\n",
        "    removed_local_models = list(local_models_config.keys())\n",
        "    \n",
        "    # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
        "    loaded_models.clear()\n",
        "    loaded_vocoder = None\n",
        "    current_model_name = None\n",
        "    \n",
        "    # ğŸ”´ ĞĞ§Ğ˜Ğ©ĞĞ•Ğœ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ›ĞĞšĞĞ›Ğ¬ĞĞ«Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ˜Ğ— ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ˜ (Ğ£Ğ”ĞĞ›Ğ¯Ğ•Ğœ Ğ˜Ğ— Ğ”Ğ ĞĞŸĞ‘ĞĞšĞ¡Ğ)\n",
        "    local_models_config.clear()\n",
        "    save_local_models()\n",
        "    \n",
        "    # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ ĞºÑÑˆ CUDA\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)\n",
        "    updated_models_config = get_all_models_config()  # ğŸ”´ Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ²ĞµÑ€Ğ½ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ MODELS_CONFIG\n",
        "    \n",
        "    # Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½ÑƒÑ Ğ¡Ğ¢ĞĞĞ”ĞĞ Ğ¢ĞĞ£Ğ® Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n",
        "    new_model = list(updated_models_config.keys())[0] if updated_models_config else None\n",
        "    \n",
        "    removed_count = len(removed_local_models)\n",
        "    status_message = f\"âœ… Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ {removed_count} Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\"\n",
        "    \n",
        "    print(status_message)\n",
        "    \n",
        "    return (\n",
        "        gr.update(value=f\"<div class='model-status model-loaded'>{status_message}</div>\", visible=True),\n",
        "        gr.update(value=\"\"),  # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "        gr.update(value=\"\"),  # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\n",
        "        gr.update(value=\"\"),  # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğµ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "        gr.update(choices=list(updated_models_config.keys()), value=new_model)  # ğŸ”´ ĞĞ‘ĞĞĞ’Ğ›Ğ¯Ğ•Ğœ Ğ”Ğ ĞĞŸĞ‘ĞĞšĞ¡ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)\n",
        "    )\n",
        "\n",
        "def remove_local_model(model_name):\n",
        "    \"\"\"Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸\"\"\"\n",
        "    global local_models_config\n",
        "    \n",
        "    if model_name in local_models_config:\n",
        "        del local_models_config[model_name]\n",
        "        save_local_models()\n",
        "        \n",
        "        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "        updated_models_config = get_all_models_config()\n",
        "        \n",
        "        # Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n",
        "        new_model = list(updated_models_config.keys())[0] if updated_models_config else None\n",
        "        \n",
        "        return (\n",
        "            gr.update(choices=list(updated_models_config.keys()), value=new_model),\n",
        "            gr.update(value=f\"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ '{model_name}' ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ°\", visible=True)\n",
        "        )\n",
        "    else:\n",
        "        return gr.update(), gr.update(value=f\"âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ '{model_name}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°\", visible=True)\n",
        "\n",
        "def check_cuda_availability():\n",
        "    \"\"\"Check if CUDA is available and raise error if not\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA is not available! This application requires GPU with CUDA support.\")\n",
        "    print(f\"CUDA is available. Using device: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "def parse_hf_path(hf_path):\n",
        "    \"\"\"ĞŸĞ°Ñ€ÑĞ¸Ñ‚ HF Ğ¿ÑƒÑ‚ÑŒ Ğ² repo_id Ğ¸ filename\"\"\"\n",
        "    if hf_path.startswith('hf://'):\n",
        "        path_parts = hf_path.replace('hf://', '').split('/')\n",
        "        if len(path_parts) >= 2:\n",
        "            # Ğ ĞµĞ¿Ğ¾ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… Ğ´Ğ²ÑƒÑ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹: namespace/repo_name\n",
        "            repo_id = '/'.join(path_parts[:2])  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°\n",
        "            # Ğ¤Ğ°Ğ¹Ğ» - Ğ²ÑĞµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸\n",
        "            filename = '/'.join(path_parts[2:])  # Ğ’ÑĞµ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°\n",
        "            return repo_id, filename\n",
        "    return None, None\n",
        "\n",
        "def add_local_model(model_path, vocab_path, model_name):\n",
        "    \"\"\"Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ\"\"\"\n",
        "    global local_models_config, MODELS_CONFIG\n",
        "    \n",
        "    if not model_path or not vocab_path or not model_name:\n",
        "        return gr.update(), gr.update(value=\"âŒ Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ²ÑĞµ Ğ¿Ğ¾Ğ»Ñ\", visible=True)\n",
        "    \n",
        "    # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ HF ĞŸĞ£Ğ¢Ğ•Ğ™\n",
        "    def validate_hf_path(file_path, file_type):\n",
        "        if file_path.startswith('hf://'):\n",
        "            repo_id, filename = parse_hf_path(file_path)\n",
        "            if not repo_id or not filename:\n",
        "                return False, f\"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ HF Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ {file_type}\"\n",
        "            \n",
        "            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° repo_id\n",
        "            if '/' not in repo_id or len(repo_id.split('/')) != 2:\n",
        "                return False, f\"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ repo_id: {repo_id}\"\n",
        "                \n",
        "            return True, None\n",
        "        else:\n",
        "            # Ğ”Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "            if os.path.exists(file_path):\n",
        "                return True, None\n",
        "            else:\n",
        "                return False, f\"âŒ Ğ¤Ğ°Ğ¹Ğ» {file_type} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {file_path}\"\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "    is_model_valid, model_error = validate_hf_path(model_path, \"Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\")\n",
        "    if not is_model_valid:\n",
        "        return gr.update(), gr.update(value=model_error, visible=True)\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\n",
        "    is_vocab_valid, vocab_error = validate_hf_path(vocab_path, \"ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\")\n",
        "    if not is_vocab_valid:\n",
        "        return gr.update(), gr.update(value=vocab_error, visible=True)\n",
        "    \n",
        "    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ\n",
        "    local_models_config[model_name] = {\n",
        "        \"repo\": \"local\",\n",
        "        \"model_file\": model_path,\n",
        "        \"vocab_file\": vocab_path,\n",
        "        \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "    }\n",
        "\n",
        "    # ğŸ”´ Ğ¡ĞĞ¥Ğ ĞĞĞ¯Ğ•Ğœ Ğ’ Ğ¤ĞĞ™Ğ›\n",
        "    save_local_models()\n",
        "    \n",
        "    # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    updated_models_config = {**MODELS_CONFIG, **local_models_config}\n",
        "    \n",
        "    # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞĞ‘Ğ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ dropdown Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°\n",
        "    return gr.update(\n",
        "        choices=list(updated_models_config.keys()),\n",
        "        value=model_name\n",
        "    ), gr.update(value=f\"âœ… Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ '{model_name}' Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ°\", visible=True)\n",
        "\n",
        "# ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°\n",
        "def check_token_validity(token):\n",
        "    \"\"\"ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°\"\"\"\n",
        "    if not token:\n",
        "        return False, \"Ğ¢Ğ¾ĞºĞµĞ½ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹\"\n",
        "    \n",
        "    # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°\n",
        "    if not token.startswith(\"hf_\"):\n",
        "        return False, \"Ğ¢Ğ¾ĞºĞµĞ½ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒÑÑ Ñ 'hf_'\"\n",
        "    \n",
        "    if len(token) < 20:\n",
        "        return False, \"Ğ¢Ğ¾ĞºĞµĞ½ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹\"\n",
        "    \n",
        "    return True, \"Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹\"\n",
        "\n",
        "# ğŸ”´ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞĞĞ¯ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯: ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ° HuggingFace\n",
        "def update_hf_token(new_token):\n",
        "    \"\"\"ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ° HuggingFace\"\"\"\n",
        "    global HF_TOKEN\n",
        "    \n",
        "    if not new_token:\n",
        "        return gr.update(value=\"âŒ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½\", visible=True)\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°\n",
        "    is_valid, validation_msg = check_token_validity(new_token)\n",
        "    if not is_valid:\n",
        "        return gr.update(value=f\"âŒ {validation_msg}\", visible=True)\n",
        "    \n",
        "    try:\n",
        "        print(f\"ğŸ”„ ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼...\")\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ\n",
        "        login(token=new_token)\n",
        "        HF_TOKEN = new_token\n",
        "        \n",
        "        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞµÑÑĞ¸Ğ¹\n",
        "        try:\n",
        "            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ĞµÑĞ»Ğ¸ ĞµÑ‘ Ğ½ĞµÑ‚\n",
        "            os.makedirs(\"/root/.cache/huggingface\", exist_ok=True)\n",
        "            \n",
        "            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ñ„Ğ°Ğ¹Ğ»\n",
        "            with open(\"/root/.cache/huggingface/token\", \"w\") as f:\n",
        "                f.write(new_token)\n",
        "            print(f\"âœ… Ğ¢Ğ¾ĞºĞµĞ½ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ Ğ² Ñ„Ğ°Ğ¹Ğ»\")\n",
        "        except Exception as file_error:\n",
        "            print(f\"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½ Ğ² Ñ„Ğ°Ğ¹Ğ»: {file_error}\")\n",
        "        \n",
        "        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ\n",
        "        os.environ[\"HF_TOKEN\"] = new_token\n",
        "        \n",
        "        print(f\"âœ… Ğ¢Ğ¾ĞºĞµĞ½ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½\")\n",
        "        return gr.update(value=\"âœ… Ğ¢Ğ¾ĞºĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½!\", visible=True)\n",
        "    \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ°: {error_msg}\")\n",
        "        \n",
        "        # Ğ‘Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…\n",
        "        if \"401\" in error_msg:\n",
        "            detailed_msg = \"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°.\"\n",
        "        elif \"400\" in error_msg:\n",
        "            detailed_msg = \"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ°.\"\n",
        "        elif \"connection\" in error_msg.lower():\n",
        "            detailed_msg = \"âŒ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ñƒ. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ.\"\n",
        "        else:\n",
        "            detailed_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)[:100]}\"\n",
        "        \n",
        "        return gr.update(value=detailed_msg, visible=True)\n",
        "\n",
        "def add_local_model_with_auth(model_path, vocab_path, model_name, hf_token=None):\n",
        "    \"\"\"Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸\"\"\"\n",
        "    global local_models_config, MODELS_CONFIG, HF_TOKEN\n",
        "    \n",
        "    if not model_path or not vocab_path or not model_name:\n",
        "        return gr.update(), gr.update(value=\"âŒ Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ²ÑĞµ Ğ¿Ğ¾Ğ»Ñ\", visible=True)\n",
        "    \n",
        "    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸Ğ»Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹\n",
        "    use_token = hf_token if hf_token else HF_TOKEN\n",
        "    \n",
        "    # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ HF ĞŸĞ£Ğ¢Ğ•Ğ™\n",
        "    def validate_hf_path(file_path, file_type):\n",
        "        if file_path.startswith('hf://'):\n",
        "            repo_id, filename = parse_hf_path(file_path)\n",
        "            if not repo_id or not filename:\n",
        "                return False, f\"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ HF Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ {file_type}\"\n",
        "            \n",
        "            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° repo_id\n",
        "            if '/' not in repo_id or len(repo_id.split('/')) != 2:\n",
        "                return False, f\"âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ repo_id: {repo_id}\"\n",
        "                \n",
        "            return True, None\n",
        "        else:\n",
        "            # Ğ”Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "            if os.path.exists(file_path):\n",
        "                return True, None\n",
        "            else:\n",
        "                return False, f\"âŒ Ğ¤Ğ°Ğ¹Ğ» {file_type} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {file_path}\"\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "    is_model_valid, model_error = validate_hf_path(model_path, \"Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\")\n",
        "    if not is_model_valid:\n",
        "        return gr.update(), gr.update(value=model_error, visible=True)\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\n",
        "    is_vocab_valid, vocab_error = validate_hf_path(vocab_path, \"ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\")\n",
        "    if not is_vocab_valid:\n",
        "        return gr.update(), gr.update(value=vocab_error, visible=True)\n",
        "    \n",
        "    # ğŸ”´ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ Ğ”Ğ›Ğ¯ Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ¯ Ğ¡ Ğ¢ĞĞšĞ•ĞĞĞœ\n",
        "    def download_hf_file_with_token(hf_path, file_type):\n",
        "        if hf_path.startswith('hf://'):\n",
        "            try:\n",
        "                repo_id, filename = parse_hf_path(hf_path)\n",
        "                if not repo_id or not filename:\n",
        "                    raise ValueError(f\"Invalid HF path: {hf_path}\")\n",
        "                \n",
        "                print(f\"Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ {file_type} Ğ¸Ğ· {repo_id}/{filename}\")\n",
        "                \n",
        "                # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼\n",
        "                downloaded_path = hf_hub_download(\n",
        "                    repo_id=repo_id,\n",
        "                    filename=filename,\n",
        "                    repo_type=\"model\",\n",
        "                    token=use_token  # â¬…ï¸ Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—Ğ£Ğ•Ğœ Ğ¢ĞĞšĞ•Ğ\n",
        "                )\n",
        "                print(f\"âœ… {file_type} ÑĞºĞ°Ñ‡Ğ°Ğ½: {downloaded_path}\")\n",
        "                return downloaded_path\n",
        "                \n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                if \"401\" in error_msg or \"403\" in error_msg:\n",
        "                    if use_token is None:\n",
        "                        raise PermissionError(f\"âŒ Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ {repo_id} Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğ¹. Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾ĞºĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸!\")\n",
        "                    else:\n",
        "                        raise PermissionError(f\"âŒ ĞĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº {repo_id}. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°.\")\n",
        "                elif \"404\" in error_msg:\n",
        "                    raise FileNotFoundError(f\"âŒ Ğ¤Ğ°Ğ¹Ğ» {filename} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ {repo_id}\")\n",
        "                else:\n",
        "                    raise e\n",
        "        else:\n",
        "            # Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ\n",
        "            if not os.path.exists(hf_path):\n",
        "                raise FileNotFoundError(f\"Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {hf_path}\")\n",
        "            return hf_path\n",
        "    \n",
        "    try:\n",
        "        # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼\n",
        "        downloaded_model = download_hf_file_with_token(model_path, \"Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\")\n",
        "        downloaded_vocab = download_hf_file_with_token(vocab_path, \"ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\")\n",
        "        \n",
        "        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³\n",
        "        local_models_config[model_name] = {\n",
        "            \"repo\": \"local\",\n",
        "            \"model_file\": downloaded_model,  # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ\n",
        "            \"vocab_file\": downloaded_vocab,\n",
        "            \"model_cfg\": dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
        "        }\n",
        "\n",
        "        # ğŸ”´ Ğ¡ĞĞ¥Ğ ĞĞĞ¯Ğ•Ğœ Ğ’ Ğ¤ĞĞ™Ğ›\n",
        "        save_local_models()\n",
        "        \n",
        "        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "        updated_models_config = {**MODELS_CONFIG, **local_models_config}\n",
        "        \n",
        "        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞĞ‘Ğ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ dropdown Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°\n",
        "        return gr.update(\n",
        "            choices=list(updated_models_config.keys()),\n",
        "            value=model_name\n",
        "        ), gr.update(value=f\"âœ… Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ '{model_name}' Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ°\", visible=True)\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)}\"\n",
        "        print(f\"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {traceback.format_exc()}\")\n",
        "        return gr.update(), gr.update(value=error_msg, visible=True)\n",
        "\n",
        "def load_model_with_progress(model_name, progress_callback=None):\n",
        "    \"\"\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°\"\"\"\n",
        "    global loaded_models, current_model_name, stop_generation\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ»Ğ°Ğ³Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸\n",
        "    if stop_generation:\n",
        "        print(\"ğŸ›‘ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ\")\n",
        "        return None\n",
        "    \n",
        "    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    all_models_config = get_all_models_config()\n",
        "    \n",
        "    # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²\n",
        "    if not model_name or model_name not in all_models_config:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "    \n",
        "    # Ğ•ÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞµÑ‘\n",
        "    if model_name in loaded_models:\n",
        "        print(f\"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ '{model_name}' ÑƒĞ¶Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°\")\n",
        "        return loaded_models[model_name]\n",
        "    \n",
        "    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "    config = all_models_config[model_name]\n",
        "    \n",
        "    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "    print(f\"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {model_name}\")\n",
        "    \n",
        "    # ğŸ”´ Ğ’Ğ«ĞĞ•Ğ¡Ğ•ĞĞ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ download_hf_file Ğ—Ğ ĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ« Ğ‘Ğ›ĞĞšĞĞ’\n",
        "    def download_hf_file(hf_path, file_type, progress_stage):\n",
        "        \"\"\"Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ°Ğ¹Ğ» Ğ¸Ğ· HF ĞµÑĞ»Ğ¸ Ğ¿ÑƒÑ‚ÑŒ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ hf://\"\"\"\n",
        "        if hf_path.startswith('hf://'):\n",
        "            if progress_callback:\n",
        "                progress_callback(f\"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° {file_type} Ğ¸Ğ· Hugging Face...\", progress_stage)\n",
        "            \n",
        "            try:\n",
        "                # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ™ ĞŸĞĞ Ğ¡Ğ˜ĞĞ“: repo_id = Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 2 Ñ‡Ğ°ÑÑ‚Ğ¸, filename = Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ\n",
        "                repo_id, filename = parse_hf_path(hf_path)\n",
        "                if not repo_id or not filename:\n",
        "                    raise ValueError(f\"Invalid HF path format: {hf_path}\")\n",
        "                \n",
        "                print(f\"Downloading {file_type} from HF: {repo_id}/{filename}\")\n",
        "\n",
        "                downloaded_path = hf_hub_download(\n",
        "                    repo_id=repo_id, \n",
        "                    filename=filename,\n",
        "                    repo_type=\"model\",\n",
        "                    token=HF_TOKEN  # â¬…ï¸ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ¯Ğ•Ğœ Ğ¢ĞĞšĞ•Ğ Ğ”Ğ›Ğ¯ Ğ—ĞĞšĞ Ğ«Ğ¢Ğ«Ğ¥ Ğ Ğ•ĞŸĞĞ—Ğ˜Ğ¢ĞĞ Ğ˜Ğ•Ğ’\n",
        "                )\n",
        "                print(f\"âœ… {file_type} downloaded to: {downloaded_path}\")\n",
        "                return downloaded_path\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Failed to download {file_type} from HF: {e}\")\n",
        "                # Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº\n",
        "                if \"404\" in str(e):\n",
        "                    raise FileNotFoundError(f\"Ğ¤Ğ°Ğ¹Ğ» {filename} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ {repo_id}\")\n",
        "                elif \"401\" in str(e):\n",
        "                    # ğŸ”´ Ğ£Ğ›Ğ£Ğ§Ğ¨Ğ•ĞĞĞĞ• Ğ¡ĞĞĞ‘Ğ©Ğ•ĞĞ˜Ğ• Ğ”Ğ›Ğ¯ Ğ—ĞĞšĞ Ğ«Ğ¢Ğ«Ğ¥ Ğ Ğ•ĞŸĞĞ—Ğ˜Ğ¢ĞĞ Ğ˜Ğ•Ğ’\n",
        "                    if HF_TOKEN is None:\n",
        "                        raise PermissionError(f\"ĞĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ {repo_id}. Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ!\")\n",
        "                    else:\n",
        "                        raise PermissionError(f\"ĞĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ {repo_id}. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°.\")\n",
        "                else:\n",
        "                    raise e\n",
        "        else:\n",
        "            # Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ - Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ\n",
        "            if not os.path.exists(hf_path):\n",
        "                raise FileNotFoundError(f\"{file_type} file not found: {hf_path}\")\n",
        "            return hf_path\n",
        "    \n",
        "    # ğŸ”´ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞĞĞ¯ Ğ›ĞĞ“Ğ˜ĞšĞ: ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ›ĞĞšĞĞ›Ğ¬ĞĞ«Ğ¥ Ğ˜ HF ĞŸĞ£Ğ¢Ğ•Ğ™\n",
        "    if config['repo'] == \"local\":\n",
        "        if progress_callback:\n",
        "            progress_callback(\"ğŸ” ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...\", 0.1)\n",
        "\n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜\n",
        "        if stop_generation:\n",
        "            print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\")\n",
        "            return None\n",
        "        \n",
        "        model_path = config['model_file']\n",
        "        vocab_path = config['vocab_file']\n",
        "        \n",
        "        # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼/Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\n",
        "        try:\n",
        "            model_path = download_hf_file(model_path, \"model\", 0.2)\n",
        "            vocab_path = download_hf_file(vocab_path, \"vocab\", 0.3)\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            raise e\n",
        "            \n",
        "        print(f\"ğŸ“ Loading local model from: {model_path}\")\n",
        "        \n",
        "    else:\n",
        "        # ğŸ”´ Ğ¡Ğ¢ĞĞĞ”ĞĞ Ğ¢ĞĞĞ¯ Ğ›ĞĞ“Ğ˜ĞšĞ Ğ”Ğ›Ğ¯ ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ Ğ˜Ğ— HUGGINGFACE\n",
        "        if progress_callback:\n",
        "            progress_callback(\"ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² HuggingFace...\", 0.1)\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜\n",
        "        if stop_generation:\n",
        "            print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² HF\")\n",
        "            return None\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ CUDA\n",
        "        check_cuda_availability()\n",
        "\n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞĞ¡Ğ›Ğ• ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ˜ CUDA\n",
        "        if stop_generation:\n",
        "            print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ CUDA\")\n",
        "            return None\n",
        "        \n",
        "        # ğŸ”´ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ¤ĞĞ™Ğ›ĞĞ’ Ğ”Ğ›Ğ¯ HF ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™\n",
        "        if progress_callback:\n",
        "            progress_callback(\"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...\", 0.3)\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞ•Ğ Ğ•Ğ” Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞĞ™\n",
        "        if stop_generation:\n",
        "            print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²\")\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            model_path = hf_hub_download(\n",
        "                repo_id=config['repo'], \n",
        "                filename=config['model_file'],\n",
        "                repo_type=\"model\"\n",
        "            )\n",
        "            vocab_path = hf_hub_download(\n",
        "                repo_id=config['repo'], \n",
        "                filename=config['vocab_file'],\n",
        "                repo_type=\"model\"\n",
        "            )\n",
        "            print(f\"âœ… Model downloaded to: {model_path}\")\n",
        "            print(f\"âœ… Vocab downloaded to: {vocab_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(\"âŒ hf_hub_download failed:\", e)\n",
        "            \n",
        "            # ğŸ”´ Ğ Ğ•Ğ—Ğ•Ğ Ğ’ĞĞĞ¯ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ¯: ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "            if progress_callback:\n",
        "                progress_callback(\"ğŸ“¦ Ğ ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğµ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...\", 0.5)\n",
        "            \n",
        "            # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞ•Ğ Ğ•Ğ” Ğ Ğ•Ğ—Ğ•Ğ Ğ’ĞĞĞ™ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ•Ğ™\n",
        "            if stop_generation:\n",
        "                print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹\")\n",
        "                return None\n",
        "            \n",
        "            try:\n",
        "                local_dir = f\"cache_{config['repo'].replace('/', '_')}\"\n",
        "                snapshot_dir = snapshot_download(\n",
        "                    repo_id=config['repo'], \n",
        "                    cache_dir=None, \n",
        "                    local_dir=local_dir,\n",
        "                    repo_type=\"model\",\n",
        "                    allow_patterns=[config['model_file'], config['vocab_file']]\n",
        "                )\n",
        "                \n",
        "                # Ğ˜Ñ‰ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ² ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸\n",
        "                possible_model = os.path.join(snapshot_dir, config['model_file'])\n",
        "                possible_vocab = os.path.join(snapshot_dir, config['vocab_file'])\n",
        "                \n",
        "                if os.path.exists(possible_model):\n",
        "                    model_path = possible_model\n",
        "                    print(f\"âœ… Model found in snapshot: {model_path}\")\n",
        "                else:\n",
        "                    print(f\"âŒ Model file not found in snapshot: {possible_model}\")\n",
        "                    \n",
        "                if os.path.exists(possible_vocab):\n",
        "                    vocab_path = possible_vocab\n",
        "                    print(f\"âœ… Vocab found in snapshot: {vocab_path}\")\n",
        "                else:\n",
        "                    print(f\"âŒ Vocab file not found in snapshot: {possible_vocab}\")\n",
        "                    \n",
        "            except Exception as snapshot_error:\n",
        "                print(\"âŒ snapshot_download also failed:\", snapshot_error)\n",
        "                # ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸\n",
        "                combined_error = f\"Original error: {e}\\nSnapshot error: {snapshot_error}\"\n",
        "                raise Exception(f\"Failed to download model: {combined_error}\")\n",
        "    \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ¤Ğ›ĞĞ“Ğ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞĞ¡Ğ›Ğ• Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜ Ğ¤ĞĞ™Ğ›ĞĞ’\n",
        "    if stop_generation:\n",
        "        print(\"ğŸ›‘ Generation stopped during file download\")\n",
        "        return None\n",
        "    \n",
        "    # ğŸ”´ Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ¡Ğ£Ğ©Ğ•Ğ¡Ğ¢Ğ’ĞĞ’ĞĞĞ˜Ğ¯ Ğ¤ĞĞ™Ğ›ĞĞ’\n",
        "    if not model_path or not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"âŒ Model file not found: {model_path}\")\n",
        "    if not vocab_path or not os.path.exists(vocab_path):\n",
        "        raise FileNotFoundError(f\"âŒ Vocab file not found: {vocab_path}\")\n",
        "    \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞ•Ğ Ğ•Ğ” Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞĞ™ Ğ’ ĞŸĞĞœĞ¯Ğ¢Ğ¬\n",
        "    if stop_generation:\n",
        "        print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ\")\n",
        "        return None\n",
        "\n",
        "    # ğŸ”´ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ’ ĞŸĞĞœĞ¯Ğ¢Ğ¬\n",
        "    if progress_callback:\n",
        "        progress_callback(\"ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ...\", 0.7)\n",
        "    \n",
        "    try:\n",
        "        print(f\"ğŸ¤– Loading model architecture from: {model_path}\")\n",
        "        print(f\"ğŸ“– Using vocab file: {vocab_path}\")\n",
        "        \n",
        "        model = load_model(DiT, config['model_cfg'], model_path, vocab_file=vocab_path)\n",
        "        print(\"âœ… Model architecture loaded successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ Failed to load model architecture: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        print(f\"Model config: {config['model_cfg']}\")\n",
        "        raise e\n",
        "\n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ¤Ğ›ĞĞ“Ğ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞĞ¡Ğ›Ğ• Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜ ĞĞ Ğ¥Ğ˜Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ«\n",
        "    if stop_generation:\n",
        "        print(\"ğŸ›‘ Generation stopped during model loading\")\n",
        "        return None\n",
        "        \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞ•Ğ Ğ•Ğ” ĞŸĞ•Ğ Ğ•ĞĞĞ¡ĞĞœ ĞĞ GPU\n",
        "    if stop_generation:\n",
        "        print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¾Ğ¼ Ğ½Ğ° GPU\")\n",
        "        return None\n",
        "        \n",
        "    # ğŸ”´ ĞŸĞ•Ğ Ğ•ĞœĞ•Ğ©Ğ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ ĞĞ GPU\n",
        "    if progress_callback:\n",
        "        progress_callback(\"ğŸš€ ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° GPU...\", 0.9)\n",
        "    \n",
        "    try:\n",
        "        device = torch.device(\"cuda\")\n",
        "        model.to(device)\n",
        "        print(f\"âœ… Model moved to CUDA: {device}\")\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° GPU\n",
        "        if next(model.parameters()).is_cuda:\n",
        "            print(\"âœ… Model parameters are on GPU\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Warning: Model parameters might not be on GPU\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ Failed to move model to GPU: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        raise e\n",
        "\n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞ•Ğ Ğ•Ğ” Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ•Ğœ Ğ’ ĞšĞ­Ğ¨\n",
        "    if stop_generation:\n",
        "        print(\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºÑÑˆ\")\n",
        "        return None\n",
        "\n",
        "    # ğŸ”´ Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ’ ĞšĞ­Ğ¨\n",
        "    loaded_models[model_name] = model\n",
        "    current_model_name = model_name\n",
        "\n",
        "    # ğŸ”´ Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ™ ĞŸĞ ĞĞ“Ğ Ğ•Ğ¡Ğ¡ Ğ˜ Ğ’Ğ«Ğ’ĞĞ”\n",
        "    if progress_callback:\n",
        "        progress_callback(\"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ°!\", 1.0)\n",
        "    \n",
        "    print(f\"ğŸ‰ Model '{model_name}' successfully loaded and cached\")\n",
        "    print(f\"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "def ensure_model(model_name, progress_callback=None):\n",
        "    \"\"\"Ensure model is loaded with progress tracking\"\"\"\n",
        "    global loaded_models\n",
        "    \n",
        "    if not model_name:\n",
        "        raise ValueError(\"Model name must be specified\")\n",
        "    \n",
        "    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    all_models_config = get_all_models_config()\n",
        "    \n",
        "    if model_name not in all_models_config:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "    \n",
        "    # Ğ•ÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞµÑ‘\n",
        "    if model_name in loaded_models:\n",
        "        return loaded_models[model_name]\n",
        "    \n",
        "    # Ğ˜Ğ½Ğ°Ñ‡Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°\n",
        "    return load_model_with_progress(model_name, progress_callback)\n",
        "\n",
        "def ensure_vocoder():\n",
        "    global loaded_vocoder\n",
        "    if loaded_vocoder is not None:\n",
        "        return loaded_vocoder\n",
        "\n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA\n",
        "    check_cuda_availability()\n",
        "\n",
        "    print(\"â³ Loading vocoder...\")\n",
        "    \n",
        "    try:\n",
        "        loaded_vocoder = load_vocoder()\n",
        "        device = torch.device(\"cuda\")\n",
        "        loaded_vocoder.to(device)\n",
        "        print(\"âœ… Vocoder loaded successfully\")\n",
        "        return loaded_vocoder\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Vocoder loading failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "def stop_generation_process():\n",
        "    \"\"\"ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ - ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ»Ğ°Ğ³\"\"\"\n",
        "    global stop_generation, generation_event, stop_requested\n",
        "    \n",
        "    print(\"ğŸ›‘ ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸...\")\n",
        "    stop_generation = True\n",
        "    stop_requested = True\n",
        "    generation_event.set()  # Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸\n",
        "    \n",
        "    # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾Ğº, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚\n",
        "    global generation_thread\n",
        "    if generation_thread and generation_thread.is_alive():\n",
        "        print(\"âš ï¸ ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸...\")\n",
        "    \n",
        "    # ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ CUDA\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    print(\"âœ… Ğ¤Ğ»Ğ°Ğ³ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½, Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ CUDA\")\n",
        "    return \"ğŸ›‘ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½...\"\n",
        "\n",
        "def reset_generation_flags():\n",
        "    \"\"\"Ğ¡Ğ±Ñ€Ğ¾Ñ Ñ„Ğ»Ğ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸\"\"\"\n",
        "    global stop_generation, generation_event, stop_requested\n",
        "    stop_generation = False\n",
        "    stop_requested = False\n",
        "    generation_event.clear()\n",
        "    print(\"ğŸ”„ Ğ¤Ğ»Ğ°Ğ³Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ±Ñ€Ğ¾ÑˆĞµĞ½Ñ‹\")\n",
        "\n",
        "def check_stop_generation():\n",
        "    \"\"\"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ»Ğ°Ğ³Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ (Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸)\"\"\"\n",
        "    global stop_generation, generation_event\n",
        "    \n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ„Ğ»Ğ°Ğ³\n",
        "    if stop_generation:\n",
        "        print(\"ğŸ›‘ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸!\")\n",
        "        raise KeyboardInterrupt(\"Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼\")\n",
        "    \n",
        "    # Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ event Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ\n",
        "    if generation_event.is_set():\n",
        "        print(\"ğŸ›‘ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸!\")\n",
        "        raise KeyboardInterrupt(\"Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼\")\n",
        "    \n",
        "    return False\n",
        "\n",
        "# --- Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° RUNorm Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ---\n",
        "print(\"Loading RUNorm (text normalizer)...\")\n",
        "try:\n",
        "    normalizer = RUNorm()\n",
        "    normalizer.load(\n",
        "        model_size=\"medium\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        workdir=\"./local_cache\"\n",
        "    )\n",
        "    \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ«Ğ™ ĞœĞ•Ğ¢ĞĞ”: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ norm()\n",
        "    test_text = \"13,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ»ĞµÑ‚\"\n",
        "    try:\n",
        "        result = normalizer.norm(test_text)\n",
        "        print(f\"âœ… RUNorm.norm SUCCESS: '{test_text}' -> '{result}'\")\n",
        "        print(\"âœ… RUNorm loaded successfully with 'norm' method\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ RUNorm.norm failed: {e}\")\n",
        "        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºÑƒ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ\n",
        "        class DummyNormalizer:\n",
        "            def norm(self, text):\n",
        "                return text\n",
        "        normalizer = DummyNormalizer()\n",
        "        print(\"âš ï¸ Using dummy normalizer due to error\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to load RUNorm: {e}\")\n",
        "    print(f\"ğŸ” RUNorm error details: {traceback.format_exc()}\")\n",
        "    \n",
        "    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºÑƒ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ\n",
        "    class DummyNormalizer:\n",
        "        def norm(self, text):\n",
        "            return text\n",
        "    normalizer = DummyNormalizer()\n",
        "    print(\"âš ï¸ Using dummy normalizer due to load error\")\n",
        "\n",
        "# --- Check CUDA ---\n",
        "print(\"Loading Silero Stress model...\")\n",
        "accentor = load_accentor()  # âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n",
        "\n",
        "# Ğ¢ĞµÑÑ‚ accentor\n",
        "test_word = \"Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚\"\n",
        "test_result = accentor(test_word)\n",
        "print(f\"âœ… Silero Stress loaded. Ğ¢ĞµÑÑ‚: '{test_word}' -> '{test_result}'\")\n",
        "\n",
        "print(\"Loading Whisper ASR model (supports Russian and English)...\")\n",
        "try:\n",
        "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "    ASR_MODEL_NAME = \"openai/whisper-medium\"  # Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾. ĞœĞ¾Ğ¶Ğ½Ğ¾: \"openai/whisper-base\", \"medium\", \"large-v3\"\n",
        "    asr_processor = WhisperProcessor.from_pretrained(ASR_MODEL_NAME)\n",
        "    asr_model = WhisperForConditionalGeneration.from_pretrained(ASR_MODEL_NAME)\n",
        "    asr_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Whisper model loaded. Supports Russian, English, and many other languages.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load Whisper model: {e}\")\n",
        "    asr_processor = None\n",
        "    asr_model = None\n",
        "\n",
        "# ================ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ˜ ĞĞ£Ğ”Ğ˜Ğ ================\n",
        "def check_audio_file(audio_path):\n",
        "    \"\"\"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ°\"\"\"\n",
        "    if not audio_path:\n",
        "        return gr.update(value=\"âŒ ĞĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ°\", visible=True)\n",
        "    \n",
        "    try:\n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "        if not os.path.exists(audio_path):\n",
        "            return gr.update(value=\"âŒ Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚\", visible=True)\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚\n",
        "        valid_extensions = ('.wav', '.mp3', '.ogg', '.flac', '.m4a', '.aac')\n",
        "        if not audio_path.lower().endswith(valid_extensions):\n",
        "            return gr.update(value=\"âŒ ĞĞµĞ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚\", visible=True)\n",
        "        \n",
        "        # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
        "        try:\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            duration = len(audio) / 1000.0  # Ğ² ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ…\n",
        "            \n",
        "            return gr.update(\n",
        "                value=f\"âœ… ĞÑƒĞ´Ğ¸Ğ¾ Ñ„Ğ°Ğ¹Ğ» ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚ĞµĞ½:<br>\"\n",
        "                      f\"â€¢ Ğ”Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: {duration:.1f} ÑĞµĞº<br>\"\n",
        "                      f\"â€¢ Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°: {audio.frame_rate} Ğ“Ñ†<br>\"\n",
        "                      f\"â€¢ ĞšĞ°Ğ½Ğ°Ğ»Ñ‹: {audio.channels}<br>\"\n",
        "                      f\"â€¢ Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {os.path.getsize(audio_path) / 1024:.1f} KB\",\n",
        "                visible=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return gr.update(value=f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾: {str(e)[:50]}\", visible=True)\n",
        "            \n",
        "    except Exception as e:\n",
        "        return gr.update(value=f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)[:50]}\", visible=True)\n",
        "# ================ ĞšĞĞĞ•Ğ¦ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ ================\n",
        "\n",
        "# ================ ĞĞĞ§ĞĞ›Ğ Ğ’Ğ¡Ğ¢ĞĞ’ĞšĞ˜ ================\n",
        "# ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Whisper\n",
        "def test_whisper_model():\n",
        "    \"\"\"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Whisper Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ¤– Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• WHISPER ĞœĞĞ”Ğ•Ğ›Ğ˜\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if asr_processor is None or asr_model is None:\n",
        "        print(\"âŒ Whisper Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ» (ÑĞ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ğ° 440 Ğ“Ñ† Ğ½Ğ° 2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹)\n",
        "        sample_rate = 16000\n",
        "        duration = 2.0\n",
        "        t = np.linspace(0, duration, int(sample_rate * duration))\n",
        "        test_audio = 0.5 * np.sin(2 * np.pi * 440 * t)  # ĞĞ¾Ñ‚Ğ° Ğ›Ñ 440 Ğ“Ñ†\n",
        "        \n",
        "        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
        "            test_file = f.name\n",
        "            sf.write(test_file, test_audio, sample_rate)\n",
        "        \n",
        "        print(\"ğŸµ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»...\")\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ\n",
        "        transcription = transcribe_audio(test_file)\n",
        "        \n",
        "        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»\n",
        "        try:\n",
        "            os.unlink(test_file)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        if transcription:\n",
        "            print(f\"âœ… Whisper Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚! Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ: '{transcription}'\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âŒ Whisper Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ¿ÑƒÑÑ‚ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Whisper: {e}\")\n",
        "        return False\n",
        "\n",
        "# ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼\n",
        "def check_all_models_before_launch():\n",
        "    \"\"\"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸš€ ĞŸĞ Ğ•Ğ”Ğ’ĞĞ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # 1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA\n",
        "    try:\n",
        "        check_cuda_availability()\n",
        "        print(\"âœ… CUDA Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"âŒ CUDA Ğ½Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: {e}\")\n",
        "        return False\n",
        "    \n",
        "    # 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° RUNorm\n",
        "    try:\n",
        "        test_text = \"13,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ»ĞµÑ‚\"\n",
        "        result = normalizer.norm(test_text)\n",
        "        print(f\"âœ… RUNorm Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚: '{test_text}' -> '{result}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ RUNorm Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚: {e}\")\n",
        "    \n",
        "    # 3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Silero Stress\n",
        "    try:\n",
        "        test_word = \"Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚\"\n",
        "        result = accentor(test_word)\n",
        "        print(f\"âœ… Silero Stress Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚: '{test_word}' -> '{result}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Silero Stress Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚: {e}\")\n",
        "    \n",
        "    # 4. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Whisper\n",
        "    whisper_ok = test_whisper_model()\n",
        "    \n",
        "    if not whisper_ok:\n",
        "        print(\"âš ï¸ Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: Whisper Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾!\")\n",
        "        print(\"   ĞÑƒĞ´Ğ¸Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ°.\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    return True\n",
        "# ================ ĞšĞĞĞ•Ğ¦ Ğ’Ğ¡Ğ¢ĞĞ’ĞšĞ˜ ================\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    if not audio_path or asr_processor is None or asr_model is None:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        # 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # 2. Ğ ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³ Ğ² 16 ĞºĞ“Ñ†\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # 3. ĞœĞ¾Ğ½Ğ¾\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # 4. Ğ’ numpy\n",
        "        input_audio = waveform.squeeze().numpy()\n",
        "\n",
        "        # 5. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ input_features (Ğ±ĞµĞ· Ğ¿Ğ°Ğ´Ğ´Ğ¸Ğ½Ğ³Ğ°)\n",
        "        inputs = asr_processor(\n",
        "            input_audio,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            # â— Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ padding Ğ¸ attention_mask â€” Ğ±ÑƒĞ´ĞµĞ¼ Ğ¿Ğ°Ğ´Ğ¸Ñ‚ÑŒ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ\n",
        "        )\n",
        "\n",
        "        input_features = inputs.input_features  # [1, 80, T], Ğ³Ğ´Ğµ T < 3000\n",
        "\n",
        "        # ğŸ”¹ ğŸ”¥ Ğ Ğ£Ğ§ĞĞĞ™ ĞŸĞĞ”Ğ”Ğ˜ĞĞ“ Ğ”Ğ 3000\n",
        "        current_length = input_features.shape[-1]\n",
        "        if current_length < 3000:\n",
        "            pad_length = 3000 - current_length\n",
        "            input_features = torch.nn.functional.pad(input_features, (0, pad_length), mode='constant', value=0)\n",
        "        elif current_length > 3000:\n",
        "            input_features = input_features[..., :3000]  # Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµĞ¼, ĞµÑĞ»Ğ¸ Ğ²Ğ´Ñ€ÑƒĞ³ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ\n",
        "\n",
        "        print(\"Input features shape:\", input_features.shape)  # Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ [1, 80, 3000]\n",
        "\n",
        "        # 6. ĞŸĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼ Ğ½Ğ° GPU\n",
        "        input_features = input_features.to(asr_model.device)\n",
        "\n",
        "        # 7. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ\n",
        "        with torch.no_grad():\n",
        "            generated_ids = asr_model.generate(\n",
        "                input_features=input_features,\n",
        "                language=None,\n",
        "                task=\"transcribe\",\n",
        "            )\n",
        "\n",
        "        # 8. Ğ”ĞµÑ†Ğ¾Ğ´Ğ¸Ğ½Ğ³\n",
        "        transcription = asr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        return transcription.strip().capitalize() + \".\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"ASR transcription failed:\", e)\n",
        "        return \"ĞÑˆĞ¸Ğ±ĞºĞ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ.\"\n",
        "# --- ĞšĞ¾Ğ½ĞµÑ† Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ---\n",
        "\n",
        "def validate_and_transcribe_audio(audio_path):\n",
        "    \"\"\"Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ° (Ğ±ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸)\"\"\"\n",
        "    if not audio_path:\n",
        "        return None, \"\", gr.update(value=\"\", visible=False)\n",
        "    \n",
        "    try:\n",
        "        # ğŸ”´ Ğ˜Ğ—ĞœĞ•ĞĞ•ĞĞ˜Ğ•: Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ ĞĞ• Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸!\n",
        "        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
        "        return audio_path, \"\", gr.update(value=\"\", visible=False)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"Validation failed:\", e)\n",
        "        return None, \"\", gr.update(value=\"âš ï¸ **ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ°ÑƒĞ´Ğ¸Ğ¾**\", visible=True)\n",
        "\n",
        "# ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸\n",
        "def manual_transcribe_audio(audio_path):\n",
        "    \"\"\"Ğ ÑƒÑ‡Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾\"\"\"\n",
        "    if not audio_path:\n",
        "        return \"\", gr.update(value=\"âŒ ĞĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ°\", visible=True)\n",
        "    \n",
        "    if asr_processor is None or asr_model is None:\n",
        "        return \"\", gr.update(value=\"âŒ Whisper Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½\", visible=True)\n",
        "    \n",
        "    try:\n",
        "        transcription = transcribe_audio(audio_path)\n",
        "        if transcription:\n",
        "            return transcription, gr.update(value=\"âœ… Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°\", visible=True)\n",
        "        else:\n",
        "            return \"\", gr.update(value=\"âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ\", visible=True)\n",
        "    except Exception as e:\n",
        "        print(f\"ĞÑˆĞ¸Ğ±ĞºĞ° Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸: {e}\")\n",
        "        return \"\", gr.update(value=f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)[:50]}\", visible=True)\n",
        "# ================ ĞšĞĞĞ•Ğ¦ Ğ—ĞĞœĞ•ĞĞ« ================\n",
        "\n",
        "# Check CUDA availability at startup\n",
        "print(\"Checking CUDA availability...\")\n",
        "try:\n",
        "    check_cuda_availability()\n",
        "except RuntimeError as e:\n",
        "    print(f\"FATAL ERROR: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# ================ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ˜ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ ================\n",
        "\n",
        "def process_text_with_accent(text):\n",
        "    \"\"\"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· silero-stress Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ÑƒĞºĞ² 'Ñ‘'\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        print(\"âŒ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸\")\n",
        "        return text\n",
        "\n",
        "    # ğŸ”´ Ğ•ÑĞ»Ğ¸ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ñ - Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
        "    if '+' in text:\n",
        "        print(\"âœ… Ğ¢ĞµĞºÑÑ‚ ÑƒĞ¶Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ\")\n",
        "        return text\n",
        "\n",
        "    try:\n",
        "        print(f\"ğŸ¯ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ°: '{text}'\")\n",
        "        \n",
        "        # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ğ½Ğµ-ÑĞ»Ğ¾Ğ²Ğ°\n",
        "        print(\"ğŸ”§ Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²ĞºĞ° Ğ½Ğ° ÑĞ»Ğ¾Ğ²Ğ°...\")\n",
        "        words_and_delimiters = re.split(r'(\\W+)', text)\n",
        "        result_parts = []\n",
        "        \n",
        "        print(f\"ğŸ” ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(words_and_delimiters)} Ñ‡Ğ°ÑÑ‚ĞµĞ¹\")\n",
        "        \n",
        "        for i, part in enumerate(words_and_delimiters):\n",
        "            if part.strip() and re.match(r'^\\w+$', part):\n",
        "                # ğŸ”´ Ğ­Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ²Ğ¾ - Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· silero-stress\n",
        "                print(f\"ğŸ”¤ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ»Ğ¾Ğ²Ğ° {i+1}: '{part}'\")\n",
        "                try:\n",
        "                    processed_word = accentor(part)\n",
        "                    print(f\"âœ… Ğ¡Ğ»Ğ¾Ğ²Ğ¾ '{part}' -> '{processed_word}'\")\n",
        "                    \n",
        "                    # ğŸ”´ ğŸ”¥ ĞšĞ›Ğ®Ğ§Ğ•Ğ’ĞĞ• Ğ˜Ğ—ĞœĞ•ĞĞ•ĞĞ˜Ğ•: Ğ’ĞĞ¡Ğ¡Ğ¢ĞĞĞĞ’Ğ›Ğ˜Ğ’ĞĞ•Ğœ ĞĞ Ğ˜Ğ“Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ• \"Ñ‘\" Ğ˜ \"Ğµ\"\n",
        "                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ÑƒĞºĞ² \"Ñ‘\" Ğ¸ \"Ğ\" Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ²Ğµ\n",
        "                    original_word = part\n",
        "                    processed_word_list = list(processed_word)\n",
        "                    \n",
        "                    # Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ²Ğµ\n",
        "                    for j, orig_char in enumerate(original_word):\n",
        "                        if j < len(processed_word_list):\n",
        "                            # Ğ•ÑĞ»Ğ¸ Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğµ Ğ±Ñ‹Ğ»Ğ° \"Ñ‘\" Ğ¸Ğ»Ğ¸ \"Ğ\", Ğ° Silero Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ğ» ĞµÑ‘ Ğ½Ğ° \"Ğµ\" Ğ¸Ğ»Ğ¸ \"Ğ•\"\n",
        "                            if (orig_char in ['Ñ‘', 'Ğ'] and \n",
        "                                processed_word_list[j].lower() == 'Ğµ'):\n",
        "                                # ğŸ”´ Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±ÑƒĞºĞ²Ñƒ \"Ñ‘\" Ğ¸Ğ»Ğ¸ \"Ğ\"\n",
        "                                processed_word_list[j] = orig_char\n",
        "                                print(f\"ğŸ”„ Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ±ÑƒĞºĞ²Ğ° '{orig_char}' Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ {j}\")\n",
        "                    \n",
        "                    processed_word = ''.join(processed_word_list)\n",
        "                    print(f\"ğŸ”„ ĞŸĞ¾ÑĞ»Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‘/Ğ: '{processed_word}'\")\n",
        "                    \n",
        "                    # ğŸ”´ ĞŸĞ ĞĞ¡Ğ¢ĞĞ• Ğ˜ ĞĞĞ”Ğ•Ğ–ĞĞĞ• Ğ’ĞĞ¡Ğ¡Ğ¢ĞĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ• Ğ Ğ•Ğ“Ğ˜Ğ¡Ğ¢Ğ Ğ\n",
        "                    if part and processed_word:\n",
        "                        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ±ÑƒĞºĞ²Ñ‹\n",
        "                        if part[0].isupper():\n",
        "                            processed_word = processed_word[0].upper() + processed_word[1:]\n",
        "                            print(f\"ğŸ”  Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€: '{processed_word}'\")\n",
        "                        else:\n",
        "                            processed_word = processed_word[0].lower() + processed_word[1:]\n",
        "                            print(f\"ğŸ”¡ Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€: '{processed_word}'\")\n",
        "                        \n",
        "                        result_parts.append(processed_word)\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑĞ»Ğ¾Ğ²Ğ¾ '{part}'\")\n",
        "                        result_parts.append(part)\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Silero stress failed for word '{part}': {e}\")\n",
        "                    result_parts.append(part)\n",
        "            else:\n",
        "                # ğŸ”´ ĞĞµ ÑĞ»Ğ¾Ğ²Ğ¾ (Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ¸ Ñ‚.Ğ´.) - Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
        "                if part.strip():\n",
        "                    print(f\"ğŸ“ ĞĞµ-ÑĞ»Ğ¾Ğ²Ğ¾: '{part}'\")\n",
        "                result_parts.append(part)\n",
        "        \n",
        "        final_result = ''.join(result_parts)\n",
        "        print(f\"ğŸ¯ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸: '{final_result}'\")\n",
        "        print(f\"ğŸ“Š Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹: '{text}' -> ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹: '{final_result}'\")\n",
        "        return final_result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Text processing failed: {e}\")\n",
        "        print(f\"ğŸ” Traceback: {traceback.format_exc()}\")\n",
        "        return text\n",
        "\n",
        "def normalize_gen_text(gen_text):\n",
        "    \"\"\"ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (silero-stress)\"\"\"\n",
        "    # Ğ‘Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ Ğº Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼\n",
        "    if gen_text and isinstance(gen_text, str):\n",
        "        if any(bad in gen_text for bad in [\"/tmp/\", \".wav\", \".mp3\", \".ogg\", \".flac\", \"/home/\", \"/root/\"]):\n",
        "            gen_text = \"\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ”„ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ Ğ”Ğ›Ğ¯ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ˜\")\n",
        "    print(f\"ğŸ“– Ğ¢ĞµĞºÑÑ‚ Ğ´Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: '{gen_text}'\")\n",
        "    \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ: Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹?\n",
        "    contains_digits = any(char.isdigit() for char in gen_text) if gen_text else False\n",
        "    \n",
        "    # Ğ¨Ğ°Ğ³ 1: ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· RUNorm (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ†Ğ¸Ñ„Ñ€Ñ‹)\n",
        "    if gen_text and contains_digits:\n",
        "        print(\"ğŸ”¢ Ğ¢ĞµĞºÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹, Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "        print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 1: ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· RUNorm...\")\n",
        "        try:\n",
        "            normalized_text = normalizer.norm(gen_text)\n",
        "            print(f\"ğŸ“ ĞŸĞ¾ÑĞ»Ğµ RUNorm: '{normalized_text}'\")\n",
        "        except Exception as norm_error:\n",
        "            print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° RUNorm: {norm_error}\")\n",
        "            normalized_text = gen_text\n",
        "    else:\n",
        "        if gen_text and not contains_digits:\n",
        "            print(\"ğŸ”¤ Ğ¢ĞµĞºÑÑ‚ Ğ½Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "        normalized_text = gen_text if gen_text else \"\"\n",
        "    \n",
        "    # Ğ¨Ğ°Ğ³ 2: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· silero-stress\n",
        "    if normalized_text:\n",
        "        print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 2: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğ¹...\")\n",
        "        processed_gen = process_text_with_accent(normalized_text)\n",
        "        print(f\"âœ… Ğ¢ĞµĞºÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: '{processed_gen}'\")\n",
        "    else:\n",
        "        processed_gen = \"\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return processed_gen\n",
        "\n",
        "def normalize_ref_text(ref_text):\n",
        "    \"\"\"ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° (RUNorm + silero-stress)\"\"\"\n",
        "    # Ğ‘Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ Ğº Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼\n",
        "    if ref_text and isinstance(ref_text, str):\n",
        "        if any(bad in ref_text for bad in [\"/tmp/\", \".wav\", \".mp3\", \".ogg\", \".flac\", \"/home/\", \"/root/\"]):\n",
        "            ref_text = \"\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ”„ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ Ğ•Ğ¤Ğ•Ğ Ğ•ĞĞ¡ĞĞĞ“Ğ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ\")\n",
        "    print(f\"ğŸ“– Ğ ĞµÑ„ĞµÑ€ĞµĞ½Ñ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: '{ref_text}'\")\n",
        "    \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ: Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹?\n",
        "    contains_digits = any(char.isdigit() for char in ref_text) if ref_text else False\n",
        "    \n",
        "    # Ğ¨Ğ°Ğ³ 1: ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· RUNorm (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ†Ğ¸Ñ„Ñ€Ñ‹)\n",
        "    if ref_text and contains_digits:\n",
        "        print(\"ğŸ”¢ Ğ¢ĞµĞºÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹, Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "        print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 1: ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· RUNorm...\")\n",
        "        try:\n",
        "            normalized_text = normalizer.norm(ref_text)\n",
        "            print(f\"ğŸ“ ĞŸĞ¾ÑĞ»Ğµ RUNorm: '{normalized_text}'\")\n",
        "        except Exception as norm_error:\n",
        "            print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° RUNorm: {norm_error}\")\n",
        "            normalized_text = ref_text\n",
        "    else:\n",
        "        if ref_text and not contains_digits:\n",
        "            print(\"ğŸ”¤ Ğ¢ĞµĞºÑÑ‚ Ğ½Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "        normalized_text = ref_text if ref_text else \"\"\n",
        "    \n",
        "    # Ğ¨Ğ°Ğ³ 2: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· silero-stress\n",
        "    if normalized_text:\n",
        "        print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 2: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğ¹...\")\n",
        "        processed_ref = process_text_with_accent(normalized_text)\n",
        "        print(f\"âœ… Ğ ĞµÑ„ĞµÑ€ĞµĞ½Ñ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: '{processed_ref}'\")\n",
        "    else:\n",
        "        processed_ref = \"\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    return processed_ref\n",
        "\n",
        "# ================ ĞĞĞ’Ğ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ Ğ¡ Ğ ĞĞ—Ğ”Ğ•Ğ›Ğ¬ĞĞ«ĞœĞ˜ Ğ¨ĞĞ“ĞĞœĞ˜ ================\n",
        "def normalize_ref_text_with_steps(ref_text):\n",
        "    \"\"\"ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ RUNorm â†’ Silero Stress\"\"\"\n",
        "    if not ref_text or not ref_text.strip():\n",
        "        return ref_text, \"âŒ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚\", \"âŒ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ”„ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ Ğ•Ğ¤Ğ•Ğ Ğ•ĞĞ¡ĞĞĞ“Ğ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ (2 Ğ¨ĞĞ“Ğ)\")\n",
        "    print(f\"ğŸ“– Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚: '{ref_text}'\")\n",
        "    \n",
        "    step1_result = \"\"\n",
        "    step2_result = \"\"\n",
        "    step1_status = \"\"\n",
        "    \n",
        "    try:\n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ: Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹?\n",
        "        contains_digits = any(char.isdigit() for char in ref_text)\n",
        "        \n",
        "        if contains_digits:\n",
        "            print(\"ğŸ”¢ Ğ¢ĞµĞºÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹, Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "            # ğŸ”´ Ğ¨ĞĞ“ 1: ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· RUNorm (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ†Ğ¸Ñ„Ñ€Ñ‹)\n",
        "            print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 1: Ğ—Ğ°Ğ¿ÑƒÑĞº RUNorm...\")\n",
        "            step1_result = normalizer.norm(ref_text)\n",
        "            print(f\"âœ… RUNorm Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: '{step1_result}'\")\n",
        "            step1_status = f\"âœ… RUNorm (Ñ†Ğ¸Ñ„Ñ€Ñ‹ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹): '{step1_result}'\"\n",
        "        else:\n",
        "            print(\"ğŸ”¤ Ğ¢ĞµĞºÑÑ‚ Ğ½Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "            step1_result = ref_text  # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm\n",
        "            step1_status = \"â­ï¸ RUNorm Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½ (Ğ½ĞµÑ‚ Ñ†Ğ¸Ñ„Ñ€)\"\n",
        "        \n",
        "        # ğŸ”´ Ğ¨ĞĞ“ 2: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Silero Stress\n",
        "        print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 2: Ğ—Ğ°Ğ¿ÑƒÑĞº Silero Stress...\")\n",
        "        step2_result = process_text_with_accent(step1_result)\n",
        "        print(f\"âœ… Silero Stress Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: '{step2_result}'\")\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(f\"ğŸ¯ Ğ˜Ñ‚Ğ¾Ğ³: '{ref_text}' â†’ '{step2_result}'\")\n",
        "        print(f\"ğŸ“ Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:\")\n",
        "        print(f\"   Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚: {len(ref_text)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²\")\n",
        "        print(f\"   ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {len(step2_result)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²\")\n",
        "        \n",
        "        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹\n",
        "        return step2_result, step1_status, f\"âœ… Silero Stress: '{step2_result}'\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return ref_text, f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)[:50]}\", \"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ° ÑˆĞ°Ğ³Ğµ 2\"\n",
        "\n",
        "def normalize_gen_text_with_steps(gen_text):\n",
        "    \"\"\"ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ RUNorm â†’ Silero Stress\"\"\"\n",
        "    if not gen_text or not gen_text.strip():\n",
        "        return gen_text, \"âŒ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚\", \"âŒ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ”„ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ˜ (2 Ğ¨ĞĞ“Ğ)\")\n",
        "    print(f\"ğŸ“– Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚: '{gen_text}'\")\n",
        "    \n",
        "    step1_result = \"\"\n",
        "    step2_result = \"\"\n",
        "    step1_status = \"\"\n",
        "    \n",
        "    try:\n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ: Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹?\n",
        "        contains_digits = any(char.isdigit() for char in gen_text)\n",
        "        \n",
        "        if contains_digits:\n",
        "            print(\"ğŸ”¢ Ğ¢ĞµĞºÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€Ñ‹, Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "            # ğŸ”´ Ğ¨ĞĞ“ 1: ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· RUNorm (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ†Ğ¸Ñ„Ñ€Ñ‹)\n",
        "            print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 1: Ğ—Ğ°Ğ¿ÑƒÑĞº RUNorm...\")\n",
        "            step1_result = normalizer.norm(gen_text)\n",
        "            print(f\"âœ… RUNorm Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: '{step1_result}'\")\n",
        "            step1_status = f\"âœ… RUNorm (Ñ†Ğ¸Ñ„Ñ€Ñ‹ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹): '{step1_result}'\"\n",
        "        else:\n",
        "            print(\"ğŸ”¤ Ğ¢ĞµĞºÑÑ‚ Ğ½Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ†Ğ¸Ñ„Ñ€, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm...\")\n",
        "            step1_result = gen_text  # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ RUNorm\n",
        "            step1_status = \"â­ï¸ RUNorm Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½ (Ğ½ĞµÑ‚ Ñ†Ğ¸Ñ„Ñ€)\"\n",
        "        \n",
        "        # ğŸ”´ Ğ¨ĞĞ“ 2: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Silero Stress\n",
        "        print(\"ğŸ”§ Ğ¨Ğ°Ğ³ 2: Ğ—Ğ°Ğ¿ÑƒÑĞº Silero Stress...\")\n",
        "        step2_result = process_text_with_accent(step1_result)\n",
        "        print(f\"âœ… Silero Stress Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: '{step2_result}'\")\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(f\"ğŸ¯ Ğ˜Ñ‚Ğ¾Ğ³: '{gen_text}' â†’ '{step2_result}'\")\n",
        "        \n",
        "        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹\n",
        "        return step2_result, step1_status, f\"âœ… {step2_result}\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return gen_text, f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)[:50]}\", \"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ° ÑˆĞ°Ğ³Ğµ 2\"\n",
        "\n",
        "def get_current_seed_display():\n",
        "    \"\"\"ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ seed Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ\"\"\"\n",
        "    global last_seed\n",
        "    # Ğ•ÑĞ»Ğ¸ last_seed ĞµÑ‰Ğµ Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹\n",
        "    if last_seed == -1:\n",
        "        last_seed = np.random.randint(0, 2**31 - 1)\n",
        "        print(f\"ğŸ² Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ seed: {last_seed}\")\n",
        "    return last_seed\n",
        "\n",
        "# ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯: ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "def update_model_loading_status(model_name):\n",
        "    \"\"\"ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\"\"\"\n",
        "    global loaded_models\n",
        "    \n",
        "    if not model_name:\n",
        "        return gr.update(value=\"<div class='model-status model-error'>âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ°</div>\", visible=True)\n",
        "    \n",
        "    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    all_models_config = get_all_models_config()\n",
        "    \n",
        "    if model_name not in all_models_config:\n",
        "        return gr.update(value=f\"<div class='model-status model-error'>âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ {model_name} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°</div>\", visible=True)\n",
        "    \n",
        "    # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚\n",
        "    if model_name in loaded_models:\n",
        "        model_type = \"ğŸ”§ Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ\" if all_models_config[model_name]['repo'] == \"local\" else \"ğŸŒ HuggingFace\"\n",
        "        return gr.update(value=f\"<div class='model-status model-loaded'>âœ… {model_name} ({model_type}) Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°</div>\", visible=True)\n",
        "    else:\n",
        "        # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑ \"Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°\" Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°\n",
        "        model_type = \"ğŸ”§ Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ\" if all_models_config[model_name]['repo'] == \"local\" else \"ğŸŒ HuggingFace\"\n",
        "        return gr.update(value=f\"<div class='model-status model-loading'>ğŸ”„ {model_name} ({model_type}) Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°</div>\", visible=True)\n",
        "\n",
        "# ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°\n",
        "def load_model_with_status(model_name, progress=gr.Progress()):\n",
        "    \"\"\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°\"\"\"\n",
        "    global loaded_models\n",
        "    \n",
        "    if not model_name:\n",
        "        return gr.update(value=\"<div class='model-status model-error'>âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ°</div>\", visible=True)\n",
        "    \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ Ğ’ ĞĞĞ§ĞĞ›Ğ•\n",
        "    try:\n",
        "        check_stop_generation()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"ğŸ›‘ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼\")\n",
        "        return gr.update(value=\"<div class='model-status model-error'>ğŸ›‘ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°</div>\", visible=True)\n",
        "    \n",
        "    # Ğ•ÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑ\n",
        "    if model_name in loaded_models:\n",
        "        return update_model_loading_status(model_name)\n",
        "    \n",
        "    try:\n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ĞŸĞ•Ğ Ğ•Ğ” ĞĞĞ§ĞĞ›ĞĞœ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜\n",
        "        check_stop_generation()\n",
        "        \n",
        "        # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸\n",
        "        progress(0.1, desc=\"ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...\")\n",
        "        \n",
        "        # ğŸ”´ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞĞ«Ğ™ CALLBACK Ğ¡ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞĞ™ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜\n",
        "        def progress_callback_with_stop(msg, p):\n",
        "            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°\n",
        "            try:\n",
        "                check_stop_generation()\n",
        "            except KeyboardInterrupt:\n",
        "                print(f\"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ: {msg}\")\n",
        "                raise\n",
        "            \n",
        "            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ\n",
        "            progress(p, desc=msg)\n",
        "        \n",
        "        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ callback, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ\n",
        "        model = load_model_with_progress(model_name, progress_callback_with_stop)\n",
        "        \n",
        "        if model:\n",
        "            # Ğ£ÑĞ¿ĞµÑˆĞ½Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°\n",
        "            return update_model_loading_status(model_name)\n",
        "        else:\n",
        "            return gr.update(value=\"<div class='model-status model-error'>âŒ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°</div>\", visible=True)\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"ğŸ›‘ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼\")\n",
        "        return gr.update(value=\"<div class='model-status model-error'>ğŸ›‘ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼</div>\", visible=True)\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {str(e)}\"\n",
        "        print(f\"Model loading error: {traceback.format_exc()}\")\n",
        "        return gr.update(value=f\"<div class='model-status model-error'>{error_msg}</div>\", visible=True)\n",
        "\n",
        "def on_model_select(model_name):\n",
        "    \"\"\"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² dropdown\"\"\"\n",
        "    global MODELS_CONFIG, local_models_config\n",
        "    \n",
        "    if not model_name:\n",
        "        return gr.update(value=\"<div class='model-status model-error'>âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ°</div>\", visible=True)\n",
        "    \n",
        "    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    all_models_config = {**MODELS_CONFIG, **local_models_config}\n",
        "    \n",
        "    if model_name not in all_models_config:\n",
        "        return gr.update(value=f\"<div class='model-status model-error'>âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ {model_name} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°</div>\", visible=True)\n",
        "    \n",
        "    # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ±ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸\n",
        "    return update_model_loading_status(model_name)\n",
        "\n",
        "def synthesize(\n",
        "    ref_audio,\n",
        "    ref_text,\n",
        "    gen_text,\n",
        "    remove_silence,\n",
        "    seed_input_value,\n",
        "    remember_seed_checkbox,\n",
        "    model_choice,\n",
        "    cross_fade_duration=0.15,\n",
        "    nfe_step=32,\n",
        "    speed=1.0,\n",
        "    sway_sampling_coef=-1,\n",
        "    cfg_strength=2,\n",
        "    audio_format=\"wav\",\n",
        "    bitrate=\"192k\",\n",
        "    progress=gr.Progress()\n",
        "):\n",
        "    global stop_generation, last_seed, remember_seed, generation_thread\n",
        "    \n",
        "    # ğŸ”´ ĞĞ‘ĞĞĞ’Ğ›Ğ¯Ğ•Ğœ Ğ¤Ğ›ĞĞ“ Ğ—ĞĞŸĞĞœĞ˜ĞĞĞĞ˜Ğ¯ Ğ¡Ğ˜Ğ”Ğ\n",
        "    remember_seed = remember_seed_checkbox\n",
        "    \n",
        "    # ğŸ”´ Ğ¡Ğ‘Ğ ĞĞ¡Ğ«Ğ’ĞĞ•Ğœ Ğ’Ğ¡Ğ• Ğ¤Ğ›ĞĞ“Ğ˜ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ˜ (Ğ—ĞĞœĞ•ĞĞ reset_stop_flag)\n",
        "    reset_generation_flags()\n",
        "    \n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ Ğ¯Ğ•Ğœ ĞĞ• Ğ‘Ğ«Ğ›Ğ Ğ›Ğ˜ Ğ£Ğ–Ğ• ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ\n",
        "    try:\n",
        "        check_stop_generation()\n",
        "    except KeyboardInterrupt:\n",
        "        gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "        current_model_status = update_model_loading_status(model_choice)  # â† Ğ”ĞĞ‘ĞĞ’Ğ¬Ğ¢Ğ• Ğ­Ğ¢Ğ£ Ğ¡Ğ¢Ğ ĞĞšĞ£\n",
        "        return None, None, ref_text, gen_text, gr.update(value=get_current_seed_display()), current_model_status\n",
        "    \n",
        "    # ğŸ”´ Ğ¡Ğ ĞĞ—Ğ£ ĞĞ‘ĞĞĞ’Ğ›Ğ¯Ğ•Ğœ Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡ ĞœĞĞ”Ğ•Ğ›Ğ˜\n",
        "    current_model_status = update_model_loading_status(model_choice)\n",
        "    \n",
        "    # ğŸ”´ Ğ ĞĞĞĞ¯Ğ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG (ÑÑ‚Ğ¾ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² Ğ²Ğ°ÑˆĞµĞ¼ ĞºĞ¾Ğ´Ğµ, Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ)\n",
        "    if stop_generation:\n",
        "        gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "        return None, None, ref_text, gen_text, gr.update(value=get_current_seed_display()), current_model_status\n",
        "    \n",
        "    # ğŸ”„ ĞĞ‘ĞĞĞ’Ğ›Ğ¯Ğ•Ğœ Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜\n",
        "    progress(0.05, desc=\"ğŸ”„ ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ°...\")\n",
        "    \n",
        "    # ğŸ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ’Ğ¥ĞĞ”ĞĞ«Ğ¥ Ğ”ĞĞĞĞ«Ğ¥\n",
        "    if not ref_audio:\n",
        "        gr.Warning(\"âŒ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾\")\n",
        "        return None, None, ref_text, gen_text, gr.update(value=get_current_seed_display()), current_model_status\n",
        "    \n",
        "    if not gen_text or not gen_text.strip():\n",
        "        gr.Warning(\"âŒ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸\")\n",
        "        return None, None, ref_text, gen_text, gr.update(value=get_current_seed_display()), current_model_status\n",
        "        \n",
        "    if not ref_text or not ref_text.strip():\n",
        "        gr.Warning(\"âŒ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚\")\n",
        "        return None, None, ref_text, gen_text, gr.update(value=get_current_seed_display()), current_model_status\n",
        "\n",
        "    # ğŸ” Ğ—ĞĞ©Ğ˜Ğ¢Ğ ĞĞ¢ ĞŸĞ£Ğ¢Ğ•Ğ™ Ğš Ğ¤ĞĞ™Ğ›ĞĞœ\n",
        "    if gen_text and isinstance(gen_text, str):\n",
        "        if any(x in gen_text for x in [\"/tmp/\", \".wav\", \".mp3\", \".ogg\", \".flac\"]):\n",
        "            gr.Warning(\"âš ï¸ ĞŸĞ¾Ğ»Ğµ 'Text to Generate' ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ.\")\n",
        "            return None, None, ref_text, gen_text, gr.update(value=get_current_seed_display()), current_model_status\n",
        "\n",
        "    # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "    if stop_generation:\n",
        "        gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "        return None, None, ref_text, gen_text, gr.update(value=get_current_seed_display()), current_model_status\n",
        "\n",
        "    # ğŸ² Ğ£Ğ¡Ğ¢ĞĞĞĞ’ĞšĞ SEED - ĞšĞ›Ğ®Ğ§Ğ•Ğ’ĞĞ• Ğ˜Ğ—ĞœĞ•ĞĞ•ĞĞ˜Ğ•!\n",
        "    current_seed = seed_input_value\n",
        "    if seed_input_value is None or seed_input_value < 0 or seed_input_value > 2**31 - 1:\n",
        "        # ğŸ”´ Ğ“Ğ•ĞĞ•Ğ Ğ˜Ğ Ğ£Ğ•Ğœ Ğ¡Ğ›Ğ£Ğ§ĞĞ™ĞĞ«Ğ™ Ğ¡Ğ˜Ğ”\n",
        "        current_seed = np.random.randint(0, 2**31 - 1)\n",
        "        print(f\"ğŸ² Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ seed: {current_seed}\")\n",
        "    \n",
        "    # ğŸ”´ Ğ—ĞĞŸĞĞœĞ˜ĞĞĞ•Ğœ Ğ¡Ğ˜Ğ” Ğ•Ğ¡Ğ›Ğ˜ Ğ’ĞšĞ›Ğ®Ğ§Ğ•ĞĞ Ğ“ĞĞ›ĞĞ§ĞšĞ\n",
        "    if remember_seed:\n",
        "        last_seed = current_seed\n",
        "        print(f\"ğŸ’¾ Seed ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {current_seed}\")\n",
        "    \n",
        "    torch.manual_seed(int(current_seed))\n",
        "    \n",
        "    # ğŸ”´ Ğ’Ğ«Ğ’ĞĞ”Ğ˜Ğœ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ® Ğ Ğ¡Ğ˜Ğ”Ğ•\n",
        "    print(f\"ğŸ² Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ seed: {current_seed} (Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ: {remember_seed})\")\n",
        "\n",
        "    # ğŸ“ ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ (Ğ‘Ğ•Ğ— ĞĞ’Ğ¢ĞĞœĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ™ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜)\n",
        "    progress(0.08, desc=\"ğŸ“ ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ°...\")\n",
        "\n",
        "    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ (Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ½Ğ¾Ğ¿ĞºÑƒ)\n",
        "    processed_ref_text = ref_text.strip() if ref_text else \"\"\n",
        "    processed_gen_text = gen_text.strip() if gen_text else \"\"\n",
        "\n",
        "    # --- âœ… Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ processed_ref_text_final Ğ”Ğ try ---\n",
        "    processed_ref_text_final = processed_ref_text\n",
        "    \n",
        "    print(f\"ğŸ¯ Ğ ĞµÑ„ĞµÑ€ĞµĞ½Ñ Ñ‚ĞµĞºÑÑ‚ (ĞºĞ°Ğº ĞµÑÑ‚ÑŒ): '{processed_ref_text}'\")\n",
        "    print(f\"ğŸ¯ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ (ĞºĞ°Ğº ĞµÑÑ‚ÑŒ): '{processed_gen_text}'\")  \n",
        "    print(\"â„¹ï¸ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ ĞºĞ½Ğ¾Ğ¿ĞºĞ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹.\")\n",
        "\n",
        "    try:\n",
        "        # --- 1. ĞĞ¢Ğ›ĞĞ–Ğ•ĞĞĞĞ¯ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ ---\n",
        "        progress(0.1, desc=\"ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...\")\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "        if stop_generation:\n",
        "            gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "        \n",
        "        # ğŸ“¥ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ ĞĞ¡ĞĞĞ’ĞĞĞ™ ĞœĞĞ”Ğ•Ğ›Ğ˜\n",
        "        progress(0.15, desc=\"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ TTS...\")\n",
        "        model = ensure_model(model_choice, lambda msg, p: progress(p, desc=msg))\n",
        "        \n",
        "        # ğŸ”´ ĞĞ‘ĞĞĞ’Ğ›Ğ¯Ğ•Ğœ Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡ ĞŸĞĞ¡Ğ›Ğ• Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜ ĞœĞĞ”Ğ•Ğ›Ğ˜\n",
        "        if model:\n",
        "            current_model_status = update_model_loading_status(model_choice)\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "        if stop_generation:\n",
        "            gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "        \n",
        "        # ğŸ”Š Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ’ĞĞšĞĞ”Ğ•Ğ Ğ (ĞĞ¢Ğ›ĞĞ–Ğ•ĞĞĞĞ¯)\n",
        "        progress(0.2, desc=\"ğŸ”Š Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ²Ğ¾ĞºĞ¾Ğ´ĞµÑ€Ğ°...\")\n",
        "        vocoder = ensure_vocoder()\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "        if stop_generation:\n",
        "            gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "            \n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: {str(e)}\"\n",
        "        gr.Warning(error_msg)\n",
        "        print(f\"Model loading error: {traceback.format_exc()}\")\n",
        "        return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "\n",
        "    # ğŸ¯ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞĞ™ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜\n",
        "    if model is None or vocoder is None:\n",
        "        gr.Warning(\"âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\")\n",
        "        return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    try:\n",
        "        # --- 2. ĞŸĞ Ğ•Ğ”ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ ĞĞ£Ğ”Ğ˜Ğ Ğ˜ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ ---\n",
        "        progress(0.3, desc=\"ğŸ”§ ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ°ÑƒĞ´Ğ¸Ğ¾...\")\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "        if stop_generation:\n",
        "            gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "        \n",
        "        try:\n",
        "            ref_audio_proc, processed_ref_text_final = preprocess_ref_audio_text(\n",
        "                ref_audio,\n",
        "                processed_ref_text,  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
        "                show_info=gr.Info\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {str(e)}\"\n",
        "            gr.Warning(error_msg)\n",
        "            traceback.print_exc()\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "        \n",
        "        # --- 3. Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ ĞĞ£Ğ”Ğ˜Ğ ---\n",
        "        progress(0.5, desc=\"ğŸµ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾...\")\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "        if stop_generation:\n",
        "            gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "        \n",
        "        try:\n",
        "            final_wave, final_sample_rate, combined_spectrogram = infer_process(\n",
        "                ref_audio_proc,\n",
        "                processed_ref_text_final,\n",
        "                processed_gen_text,  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
        "                model,\n",
        "                vocoder,\n",
        "                cross_fade_duration=cross_fade_duration,\n",
        "                nfe_step=nfe_step,\n",
        "                speed=speed,\n",
        "                sway_sampling_coef=sway_sampling_coef,\n",
        "                cfg_strength=cfg_strength,\n",
        "                show_info=gr.Info,\n",
        "                progress=progress,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾: {str(e)}\"\n",
        "            gr.Warning(error_msg)\n",
        "            traceback.print_exc()\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "        if stop_generation:\n",
        "            gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "\n",
        "        # --- 4. Ğ£Ğ”ĞĞ›Ğ•ĞĞ˜Ğ• Ğ¢Ğ˜Ğ¨Ğ˜ĞĞ« (ĞĞŸĞ¦Ğ˜ĞĞĞĞ›Ğ¬ĞĞ) ---\n",
        "        if remove_silence:\n",
        "            progress(0.7, desc=\"ğŸ”‡ Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸ÑˆĞ¸Ğ½Ñ‹...\")\n",
        "            \n",
        "            # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "            if stop_generation:\n",
        "                gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "                return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "            \n",
        "            try:\n",
        "                with tempfile.NamedTemporaryFile(suffix=\".wav\", **tempfile_kwargs) as f:\n",
        "                    temp_path = f.name\n",
        "                    sf.write(temp_path, final_wave, final_sample_rate)\n",
        "                    remove_silence_for_generated_wav(temp_path)\n",
        "                    final_wave_tensor, _ = torchaudio.load(temp_path)\n",
        "                    final_wave = final_wave_tensor.squeeze().cpu().numpy()\n",
        "            except Exception as e:\n",
        "                print(\"âš ï¸ Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸ÑˆĞ¸Ğ½Ñ‹ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ:\", e)\n",
        "                # ĞĞµ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
        "\n",
        "        # --- 5. Ğ­ĞšĞ¡ĞŸĞĞ Ğ¢ Ğ’ Ğ¤ĞĞ ĞœĞĞ¢ ---\n",
        "        progress(0.8, desc=\"ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°...\")\n",
        "        \n",
        "        # ğŸ”´ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ STOP_FLAG\n",
        "        if stop_generation:\n",
        "            gr.Warning(\"ğŸ›‘ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°\")\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "\n",
        "        # ğŸµ ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ ĞĞ£Ğ”Ğ˜Ğ Ğ”Ğ›Ğ¯ Ğ­ĞšĞ¡ĞŸĞĞ Ğ¢Ğ\n",
        "        safe_kwargs = {k: v for k, v in tempfile_kwargs.items() if k != 'delete'}\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix=f\".{audio_format}\", delete=False, **safe_kwargs) as tmp:\n",
        "            temp_output_path = tmp.name\n",
        "\n",
        "        wave = final_wave\n",
        "        if wave.ndim == 1:\n",
        "            channels = 1\n",
        "        else:\n",
        "            channels = min(wave.shape[0], 2)\n",
        "            wave = wave[:channels].T\n",
        "\n",
        "        # ğŸ”§ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ ĞĞ£Ğ”Ğ˜Ğ\n",
        "        if np.max(np.abs(wave)) == 0:\n",
        "            wave_int16 = np.zeros(wave.shape, dtype=np.int16)\n",
        "        else:\n",
        "            wave_int16 = np.int16(wave / np.max(np.abs(wave)) * 32767)\n",
        "\n",
        "        # ğŸ’¾ Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ• Ğ’ Ğ’Ğ«Ğ‘Ğ ĞĞĞĞĞœ Ğ¤ĞĞ ĞœĞĞ¢Ğ•\n",
        "        audio_segment = AudioSegment(\n",
        "            wave_int16.tobytes(),\n",
        "            frame_rate=final_sample_rate,\n",
        "            sample_width=2,\n",
        "            channels=channels\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            if audio_format == \"mp3\":\n",
        "                audio_segment.export(temp_output_path, format=\"mp3\", bitrate=bitrate)\n",
        "            elif audio_format == \"ogg\":\n",
        "                audio_segment.export(temp_output_path, format=\"ogg\", bitrate=bitrate)\n",
        "            elif audio_format == \"flac\":\n",
        "                audio_segment.export(temp_output_path, format=\"flac\")\n",
        "            else:\n",
        "                audio_segment.export(temp_output_path, format=\"wav\")\n",
        "        except Exception as e:\n",
        "            error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾: {str(e)}\"\n",
        "            gr.Warning(error_msg)\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "\n",
        "        # ğŸ“ Ğ¡ĞĞ—Ğ”ĞĞĞ˜Ğ• Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ“Ğ Ğ¤ĞĞ™Ğ›Ğ\n",
        "        timestamp = int(time.time())\n",
        "        safe_text = f\"audio_{timestamp}\"\n",
        "        final_output_path = os.path.join(os.path.dirname(temp_output_path), f\"{safe_text}.{audio_format}\")\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(final_output_path):\n",
        "                os.remove(final_output_path)\n",
        "            os.rename(temp_output_path, final_output_path)\n",
        "        except Exception as e:\n",
        "            error_msg = f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°: {str(e)}\"\n",
        "            gr.Warning(error_msg)\n",
        "            return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "\n",
        "        # --- 6. Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ• Ğ¡ĞŸĞ•ĞšĞ¢Ğ ĞĞ“Ğ ĞĞœĞœĞ« ---\n",
        "        progress(0.9, desc=\"ğŸ“Š Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹...\")\n",
        "\n",
        "        spectrogram_path = None\n",
        "        try:\n",
        "            # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ delete=False Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False, **tempfile_kwargs) as tmp_spectrogram:\n",
        "                spectrogram_path = tmp_spectrogram.name\n",
        "            \n",
        "            # ğŸ”´ Ğ¯Ğ²Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ\n",
        "            save_spectrogram(combined_spectrogram, spectrogram_path)\n",
        "            print(f\"âœ… Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°: {spectrogram_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ: {e}\")\n",
        "            spectrogram_path = None\n",
        "\n",
        "        progress(1.0, desc=\"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!\")\n",
        "\n",
        "        # ğŸ”´ Ğ”Ğ•Ğ‘ĞĞ“ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ¯\n",
        "        print(\"=\" * 50)\n",
        "        print(\"ğŸ¯ Ğ”Ğ•Ğ‘ĞĞ“ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ¯ Ğ Ğ¤ĞĞ™Ğ›ĞĞ¥:\")\n",
        "        print(f\"ğŸµ ĞÑƒĞ´Ğ¸Ğ¾ Ñ„Ğ°Ğ¹Ğ»: {final_output_path}\")\n",
        "        print(f\"ğŸµ ĞÑƒĞ´Ğ¸Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚: {os.path.exists(final_output_path)}\")\n",
        "\n",
        "        print(f\"ğŸ“Š Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°: {spectrogram_path}\")\n",
        "        if spectrogram_path:\n",
        "            exists = os.path.exists(spectrogram_path)\n",
        "            print(f\"ğŸ“Š Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚: {exists}\")\n",
        "            if exists:\n",
        "                size = os.path.getsize(spectrogram_path)\n",
        "                print(f\"ğŸ“Š Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹: {size} bytes\")\n",
        "        else:\n",
        "            print(\"ğŸ“Š Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°: None\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # ğŸ‰ Ğ’ĞĞ—Ğ’Ğ ĞĞ¢ Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞĞ“Ğ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ\n",
        "        return (\n",
        "            final_output_path,\n",
        "            spectrogram_path,\n",
        "            processed_ref_text_final,  # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
        "            processed_gen_text,        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ\n",
        "            gr.update(value=current_seed),\n",
        "            current_model_status\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {str(e)}\"\n",
        "        print(f\"Unexpected error in synthesize: {traceback.format_exc()}\")\n",
        "        gr.Warning(error_msg)\n",
        "        return None, None, processed_ref_text, processed_gen_text, gr.update(value=current_seed), current_model_status\n",
        "\n",
        "    finally:\n",
        "        # ğŸ§¹ ĞĞ§Ğ˜Ğ¡Ğ¢ĞšĞ ĞŸĞĞœĞ¯Ğ¢Ğ˜ (Ğ•Ğ¡Ğ›Ğ˜ ĞĞ• Ğ‘Ğ«Ğ›Ğ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜)\n",
        "        if not stop_generation:\n",
        "            try:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "            except Exception as e:\n",
        "                print(\"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸:\", e)\n",
        "                \n",
        "# ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "def update_model_check_status():\n",
        "    \"\"\"ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\"\"\"\n",
        "    try:\n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "        whisper_ok = test_whisper_model() if asr_processor else False\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ RUNorm\n",
        "        runorm_ok = False\n",
        "        try:\n",
        "            test_text = \"13,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ»ĞµÑ‚\"\n",
        "            result = normalizer.norm(test_text)\n",
        "            runorm_ok = True\n",
        "        except:\n",
        "            runorm_ok = False\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Silero Stress\n",
        "        silero_ok = False\n",
        "        try:\n",
        "            test_word = \"Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚\"\n",
        "            result = accentor(test_word)\n",
        "            silero_ok = True\n",
        "        except:\n",
        "            silero_ok = False\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ CUDA\n",
        "        cuda_ok = torch.cuda.is_available()\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ HuggingFace\n",
        "        hf_auth_ok = HF_TOKEN is not None\n",
        "        \n",
        "        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ HTML ÑÑ‚Ğ°Ñ‚ÑƒÑ\n",
        "        html = \"\"\"\n",
        "        <div style=\"background: #f8f9fa; padding: 15px; border-radius: 8px;\">\n",
        "            <h3>ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹:</h3>\n",
        "            <ul style=\"list-style: none; padding: 0;\">\n",
        "                <li>ğŸ¯ <strong>CUDA:</strong> {}</li>\n",
        "                <li>ğŸ” <strong>HuggingFace Auth:</strong> {}</li>\n",
        "                <li>ğŸ¤– <strong>Whisper ASR:</strong> {}</li>\n",
        "                <li>ğŸ“ <strong>RUNorm:</strong> {}</li>\n",
        "                <li>ğŸ”Š <strong>Silero Stress:</strong> {}</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        \"\"\".format(\n",
        "            \"âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½\" if cuda_ok else \"âŒ ĞĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½\",\n",
        "            \"âœ… ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½\" if hf_auth_ok else \"âŒ ĞĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ°\",\n",
        "            \"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²\" if whisper_ok else \"âŒ ĞĞµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½\",\n",
        "            \"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²\" if runorm_ok else \"âš ï¸ Ğ§Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾\",\n",
        "            \"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²\" if silero_ok else \"âš ï¸ Ğ§Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾\"\n",
        "        )\n",
        "        \n",
        "        return gr.update(value=html)\n",
        "    except Exception as e:\n",
        "        return gr.update(value=f\"<div class='model-status model-error'>âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸: {str(e)[:100]}</div>\")\n",
        "\n",
        "# ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ-Ğ¾Ğ±ĞµÑ€Ñ‚ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ°\n",
        "def update_token_handler(new_token):\n",
        "    return update_hf_token(new_token)\n",
        "\n",
        "with gr.Blocks(title=\"ESpeech-TTS\", css=\"\"\"\n",
        "    .error-markdown {\n",
        "        color: red !important;\n",
        "        font-weight: bold;\n",
        "        text-align: center;\n",
        "        font-size: 18px !important;\n",
        "        margin: 10px 0;\n",
        "        animation: fadeIn 0.3s, fadeOut 0.3s 4.7s forwards;\n",
        "    }\n",
        "    .model-status {\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        margin: 10px 0;\n",
        "        text-align: center;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "               \n",
        "    /* ğŸ”´ Ğ£Ğ’Ğ•Ğ›Ğ˜Ğ§Ğ•ĞĞĞ«Ğ• Ğ¡Ğ¢Ğ˜Ğ›Ğ˜ Ğ”Ğ›Ğ¯ ĞšĞĞĞŸĞĞš ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ */\n",
        "    .norm-button {\n",
        "        font-size: 16px !important;  /* â¬…ï¸ Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ÑˆÑ€Ğ¸Ñ„Ñ‚ */\n",
        "        padding: 12px 24px !important;  /* â¬…ï¸ Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¾Ñ‚ÑÑ‚ÑƒĞ¿Ñ‹ */\n",
        "        background: linear-gradient(45deg, #007bff, #0056b3) !important;\n",
        "        color: white !important;\n",
        "        font-weight: bold !important;\n",
        "        margin: 10px 0 !important;  /* â¬…ï¸ Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ»Ğ¸ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚ÑÑ‚ÑƒĞ¿Ñ‹ */\n",
        "        border-radius: 8px !important;  /* â¬…ï¸ Ğ¡ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ³Ğ»Ñ‹ */\n",
        "    }\n",
        "    .norm-button:hover {\n",
        "        background: linear-gradient(45deg, #0056b3, #003d82) !important;\n",
        "        transform: translateY(-2px) !important;\n",
        "        box-shadow: 0 4px 8px rgba(0, 86, 179, 0.3) !important;\n",
        "    }\n",
        "    \n",
        "    /* ğŸ”´ Ğ¡Ğ¢Ğ˜Ğ›Ğ˜ Ğ”Ğ›Ğ¯ ĞŸĞĞ›Ğ•Ğ™ ĞŸĞ ĞĞ“Ğ Ğ•Ğ¡Ğ¡Ğ */\n",
        "    .step-result-box {\n",
        "        background: #f8f9fa !important;\n",
        "        border-left: 4px solid #28a745 !important;\n",
        "        border-radius: 5px !important;\n",
        "        padding: 10px !important;\n",
        "        margin: 5px 0 !important;\n",
        "    }\n",
        "    .step-label {\n",
        "        font-weight: bold;\n",
        "        color: #155724;\n",
        "    }\n",
        "    .step-text {\n",
        "        color: #0c5460;\n",
        "        font-style: italic;\n",
        "        font-size: 14px;\n",
        "        word-break: break-word;\n",
        "    }\n",
        "    \n",
        "    /* ğŸ”´ Ğ¡Ğ¢Ğ˜Ğ›Ğ˜ Ğ”Ğ›Ğ¯ ĞšĞĞĞŸĞšĞ˜ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ */\n",
        "    .stop-button {\n",
        "        background: linear-gradient(45deg, #dc3545, #c82333) !important;\n",
        "        color: white !important;\n",
        "        font-weight: bold !important;\n",
        "        border: none !important;\n",
        "        box-shadow: 0 4px 6px rgba(220, 53, 69, 0.3) !important;\n",
        "    }\n",
        "    .stop-button:hover {\n",
        "        background: linear-gradient(45deg, #c82333, #a71d2a) !important;\n",
        "        transform: translateY(-2px) !important;\n",
        "        box-shadow: 0 6px 8px rgba(220, 53, 69, 0.4) !important;\n",
        "    }               \n",
        "    .model-loaded {\n",
        "        background: #d4edda;\n",
        "        color: #155724;\n",
        "        border: 1px solid #c3e6cb;\n",
        "    }\n",
        "    .model-error {\n",
        "        background: #f8d7da;\n",
        "        color: #721c24;\n",
        "        border: 1px solid #f5c6cb;\n",
        "    }\n",
        "    .model-loading {\n",
        "        background: #fff3cd;\n",
        "        color: #856404;\n",
        "        border: 1px solid #ffeaa7;\n",
        "    }\n",
        "    .loading-status {\n",
        "        padding: 10px;\n",
        "        margin: 10px 0;\n",
        "        border-radius: 5px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .local-model-input {\n",
        "        background: #f8f9fa;\n",
        "        padding: 15px;\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid #dee2e6;\n",
        "        margin: 10px 0;\n",
        "    }\n",
        "    @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }\n",
        "    @keyframes fadeOut { from { opacity: 1; } to { opacity: 0; } }\n",
        "    \"\"\") as app:\n",
        "    gr.Markdown(\"# ESpeech-TTS\")\n",
        "\n",
        "    # ğŸ”´ğŸ”´ğŸ”´ Ğ‘Ğ›ĞĞš Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡Ğ ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ ğŸ”´ğŸ”´ğŸ”´\n",
        "    with gr.Accordion(\"ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\", open=False):\n",
        "        model_check_status = gr.HTML(\n",
        "            value=\"<div class='model-status model-loading'>ğŸ”„ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...</div>\"\n",
        "        )\n",
        "        check_models_btn = gr.Button(\"ğŸ”„ ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\", variant=\"secondary\")\n",
        "\n",
        "    # ğŸ”´ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ¯ Ğ ĞĞ’Ğ¢ĞĞ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜\n",
        "    auth_status_text = \"âœ… ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ HuggingFace Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ°!\" if HF_TOKEN else \"âš ï¸ ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ HuggingFace Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ°. Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹.\"\n",
        "    gr.Markdown(f\"**ğŸ” Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:** {auth_status_text}\")\n",
        "\n",
        "    gr.Markdown(\"ğŸ’¡ **Ğ¡Ğ¾Ğ²ĞµÑ‚:** Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ» '+' Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ñ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 'Ğ¿Ñ€Ğ¸Ğ²+ĞµÑ‚')\")\n",
        "    gr.Markdown(\"ğŸ² **Ğ¡Ğ¾Ğ²ĞµÑ‚:** Seed Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ¿Ğ¾Ğ´Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞµĞ±Ñ ÑƒĞ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ€Ğ°Ğ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ seed\")\n",
        "    gr.Markdown(\"ğŸš€ **CUDA Required:** This application requires GPU with CUDA support\")\n",
        "\n",
        "    # ğŸ”´ ĞĞĞ’Ğ«Ğ™ Ğ‘Ğ›ĞĞš: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    with gr.Accordion(\"ğŸ”§ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\", open=False):\n",
        "        # ğŸ”´ Ğ‘Ğ›ĞĞš Ğ”Ğ›Ğ¯ Ğ¢ĞĞšĞ•ĞĞ\n",
        "        gr.Markdown(\"### ğŸ” ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ° HuggingFace\")\n",
        "        gr.Markdown(\"Ğ¢Ğ¾ĞºĞµĞ½ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ½Ğ° HuggingFace\")\n",
        "        \n",
        "        with gr.Row():\n",
        "            hf_token_input = gr.Textbox(\n",
        "                label=\"HuggingFace Token\",\n",
        "                placeholder=\"hf_xxxxxxxxxxxxxxxxxxxx\",\n",
        "                type=\"password\",  # âš ï¸ Ğ¡ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ²Ğ¾Ğ´\n",
        "                scale=4\n",
        "            )\n",
        "            update_token_btn = gr.Button(\"ğŸ” ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½\", variant=\"secondary\", scale=1)\n",
        "            token_status = gr.HTML(visible=False)\n",
        "        \n",
        "        gr.Markdown(\"### ğŸ“ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\")\n",
        "        with gr.Row():\n",
        "            local_model_path = gr.Textbox(\n",
        "                label=\"ĞŸÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\",\n",
        "                placeholder=\"hf://SWivid/F5-TTS/F5TTS_v1_Base/model_1250000.safetensors Ğ¸Ğ»Ğ¸ /path/to/model.safetensors\",\n",
        "                scale=3\n",
        "            )\n",
        "            local_vocab_path = gr.Textbox(\n",
        "                label=\"ĞŸÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ\", \n",
        "                placeholder=\"hf://SWivid/F5-TTS/F5TTS_v1_Base/vocab.txt Ğ¸Ğ»Ğ¸ /path/to/vocab.txt\",\n",
        "                scale=3\n",
        "            )\n",
        "            local_model_name = gr.Textbox(\n",
        "                label=\"Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\",\n",
        "                placeholder=\"My Local Model\",\n",
        "                scale=2\n",
        "            )\n",
        "        with gr.Row():\n",
        "            add_local_model_btn = gr.Button(\"â• Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\", variant=\"secondary\")\n",
        "            local_model_status = gr.HTML(visible=False)\n",
        "\n",
        "    # --- ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ™ Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡ ĞœĞĞ”Ğ•Ğ›Ğ˜ ---\n",
        "    all_models = get_all_models_config()\n",
        "    initial_model = list(all_models.keys())[0] if all_models else \"ESpeech-TTS-1_SFT-95K\"\n",
        "    initial_status = update_model_loading_status(initial_model)\n",
        "\n",
        "    model_status = gr.HTML(\n",
        "        value=initial_status[\"value\"],\n",
        "        visible=initial_status[\"visible\"]\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # --- Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ---\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=list(all_models.keys()),\n",
        "                value=initial_model,\n",
        "                label=\"Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\",\n",
        "                info=\"ğŸŒ HuggingFace Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ ğŸ”§ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\"\n",
        "            )\n",
        "            \n",
        "            # --- ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ¯ ĞšĞĞĞŸĞšĞ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜ ĞœĞĞ”Ğ•Ğ›Ğ˜ ---\n",
        "            load_model_btn = gr.Button(\"ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\", variant=\"secondary\")\n",
        "            clear_models_btn = gr.Button(\"ğŸ—‘ï¸ ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\", variant=\"stop\")\n",
        "            \n",
        "        with gr.Column():\n",
        "            ref_audio_input = gr.Audio(label=\"Reference Audio\", type=\"filepath\")\n",
        "\n",
        "            with gr.Row():\n",
        "                check_audio_btn = gr.Button(\"ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾\", variant=\"secondary\")\n",
        "                transcribe_btn = gr.Button(\"ğŸ¤ Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾\", variant=\"secondary\")\n",
        "            \n",
        "            audio_check_status = gr.HTML(visible=False)\n",
        "            \n",
        "            ref_text_input = gr.Textbox(\n",
        "                label=\"Reference Text\",\n",
        "                lines=2,\n",
        "                placeholder=\"Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸\"\n",
        "            )\n",
        "            \n",
        "            # ğŸ”´ ĞšĞĞĞŸĞšĞ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ Ğ Ğ•Ğ¤Ğ•Ğ Ğ•ĞĞ¡ĞĞĞ“Ğ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ (ĞŸĞĞ” Ğ˜ĞĞŸĞ£Ğ¢ĞĞœ)\n",
        "            normalize_ref_btn = gr.Button(\"ğŸ“ ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ (RUNorm â†’ Silero)\", \n",
        "                                        variant=\"secondary\", \n",
        "                                        size=\"lg\",  # â¬…ï¸ Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€\n",
        "                                        elem_classes=\"norm-button\")\n",
        "\n",
        "        with gr.Column():\n",
        "            gen_text_input = gr.Textbox(\n",
        "                label=\"Text to Generate\",\n",
        "                lines=5,\n",
        "                max_lines=20,\n",
        "                placeholder=\"Enter text to synthesize...\"\n",
        "            )\n",
        "    \n",
        "            # ğŸ”´ ĞšĞĞĞŸĞšĞ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ˜ (ĞŸĞĞ” Ğ˜ĞĞŸĞ£Ğ¢ĞĞœ)\n",
        "            normalize_gen_btn = gr.Button(\"âœï¸ ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ (RUNorm â†’ Silero)\", \n",
        "                                        variant=\"secondary\", \n",
        "                                        size=\"lg\",  # â¬…ï¸ Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€\n",
        "                                        elem_classes=\"norm-button\")\n",
        "\n",
        "    # ğŸ”´ ĞŸĞ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°\n",
        "    with gr.Accordion(\"ğŸ“Š ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°\", open=False):\n",
        "        ref_norm_step1 = gr.Textbox(label=\"Ğ¨Ğ°Ğ³ 1: RUNorm Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\", interactive=False, visible=True)\n",
        "        ref_norm_step2 = gr.Textbox(label=\"Ğ¨Ğ°Ğ³ 2: Silero Stress Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\", interactive=False, visible=True)\n",
        "\n",
        "    # ğŸ”´ ĞŸĞ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸\n",
        "    with gr.Accordion(\"ğŸ“Š ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°\", open=False):\n",
        "        gen_norm_step1 = gr.Textbox(label=\"Ğ¨Ğ°Ğ³ 1: RUNorm Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\", interactive=False, visible=True)\n",
        "        gen_norm_step2 = gr.Textbox(label=\"Ğ¨Ğ°Ğ³ 2: Silero Stress Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\", interactive=False, visible=True)\n",
        "\n",
        "\n",
        "    # --- ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ• ĞĞ‘Ğ ĞĞ‘ĞĞ¢Ğ§Ğ˜ĞšĞ˜ Ğ”Ğ›Ğ¯ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ˜ ĞœĞĞ”Ğ•Ğ›Ğ˜ ---\n",
        "    def on_model_select(model_name):\n",
        "        \"\"\"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² dropdown\"\"\"\n",
        "        if model_name:\n",
        "            return update_model_loading_status(model_name)\n",
        "        return gr.update(value=\"<div class='model-status model-error'>âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ°</div>\", visible=True)\n",
        "\n",
        "    # ================ ĞĞ‘Ğ ĞĞ‘ĞĞ¢Ğ§Ğ˜ĞšĞ˜ ĞšĞĞĞŸĞĞš ================\n",
        "    \n",
        "    # 1. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
        "    check_audio_btn.click(\n",
        "        fn=check_audio_file,\n",
        "        inputs=[ref_audio_input],\n",
        "        outputs=[audio_check_status]\n",
        "    )\n",
        "    \n",
        "    # 2. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    check_models_btn.click(\n",
        "        fn=update_model_check_status,\n",
        "        outputs=[model_check_status]\n",
        "    )\n",
        "\n",
        "    # 3. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ°\n",
        "    update_token_btn.click(\n",
        "        fn=update_token_handler,\n",
        "        inputs=hf_token_input,\n",
        "        outputs=token_status\n",
        "    )\n",
        "\n",
        "    # 4. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "    add_local_model_btn.click(\n",
        "        fn=add_local_model_with_auth,  # â¬…ï¸ ĞĞĞ’ĞĞ¯ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ Ğ¡ ĞĞ’Ğ¢ĞĞ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ•Ğ™\n",
        "        inputs=[local_model_path, local_vocab_path, local_model_name, hf_token_input],\n",
        "        outputs=[model_choice, local_model_status]\n",
        "    )\n",
        "\n",
        "    # 5. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\n",
        "    clear_models_btn.click(\n",
        "        fn=clear_loaded_models,\n",
        "        outputs=[model_status, local_model_path, local_vocab_path, local_model_name, model_choice]\n",
        "    )\n",
        "\n",
        "    # 6. ĞĞ²Ñ‚Ğ¾Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ - Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡\n",
        "    model_choice.change(\n",
        "        fn=on_model_select,\n",
        "        inputs=model_choice,\n",
        "        outputs=model_status\n",
        "    )\n",
        "\n",
        "    # 7. ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸\n",
        "    load_model_btn.click(\n",
        "        fn=load_model_with_status,\n",
        "        inputs=[model_choice],\n",
        "        outputs=model_status\n",
        "    )\n",
        "\n",
        "    # 8. ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸\n",
        "    transcribe_btn.click(\n",
        "        fn=manual_transcribe_audio,\n",
        "        inputs=[ref_audio_input],\n",
        "        outputs=[ref_text_input, audio_check_status]  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ\n",
        "    )\n",
        "\n",
        "    # ğŸ”´ ĞĞ‘Ğ ĞĞ‘ĞĞ¢Ğ§Ğ˜Ğš Ğ”Ğ›Ğ¯ ĞšĞĞĞŸĞšĞ˜ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ Ğ Ğ•Ğ¤Ğ•Ğ Ğ•ĞĞ¡Ğ\n",
        "    normalize_ref_btn.click(\n",
        "        fn=normalize_ref_text_with_steps,\n",
        "        inputs=[ref_text_input],\n",
        "        outputs=[ref_text_input, ref_norm_step1, ref_norm_step2]\n",
        "    )\n",
        "\n",
        "    # ğŸ”´ ĞĞ‘Ğ ĞĞ‘ĞĞ¢Ğ§Ğ˜Ğš Ğ”Ğ›Ğ¯ ĞšĞĞĞŸĞšĞ˜ ĞĞĞ ĞœĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ˜\n",
        "    normalize_gen_btn.click(\n",
        "        fn=normalize_gen_text_with_steps,\n",
        "        inputs=[gen_text_input],\n",
        "        outputs=[gen_text_input, gen_norm_step1, gen_norm_step2]\n",
        "    )\n",
        "\n",
        "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "        with gr.Row():\n",
        "            # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ seed\n",
        "            seed_input = gr.Number(label=\"Seed (random)\", value=get_current_seed_display(), precision=0)\n",
        "            remember_seed_checkbox = gr.Checkbox(label=\"ğŸ’¾ Ğ—Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ seed\", value=False)\n",
        "            remove_silence = gr.Checkbox(label=\"Remove Silences\", value=False)\n",
        "        with gr.Row():\n",
        "            speed_slider = gr.Slider(label=\"Speed\", minimum=0.3, maximum=2.0, value=1.0, step=0.1)\n",
        "            nfe_slider = gr.Slider(label=\"NFE Steps\", minimum=4, maximum=64, value=48, step=2)\n",
        "        with gr.Row():\n",
        "            cross_fade_slider = gr.Slider(label=\"Cross-Fade Duration (s)\", minimum=0.0, maximum=1.0, value=0.15, step=0.01)\n",
        "            sway_sampling_slider = gr.Slider(label=\"Sway Sampling Coef\", minimum=-1, maximum=1, value=-1, step=0.1)\n",
        "        with gr.Row():\n",
        "            cfg_strength_slider = gr.Slider(label=\"CFG Strength\", minimum=0.5, maximum=5.0, value=2.0, step=0.1)\n",
        "        with gr.Row():\n",
        "            audio_format = gr.Radio([\"wav\", \"mp3\", \"ogg\", \"flac\"], label=\"Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚\", value=\"wav\")\n",
        "            bitrate = gr.Radio([\"128k\", \"192k\", \"320k\"], label=\"Ğ‘Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚ (mp3/ogg)\", value=\"192k\", visible=lambda fmt: fmt in [\"mp3\", \"ogg\"])\n",
        "\n",
        "    # --- ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° ---\n",
        "    def update_bitrate_visibility(audio_format):\n",
        "        return gr.update(visible=audio_format in [\"mp3\", \"ogg\"])\n",
        "\n",
        "    audio_format.change(\n",
        "        update_bitrate_visibility,\n",
        "        inputs=audio_format,\n",
        "        outputs=bitrate\n",
        "    )\n",
        "\n",
        "    # --- ğŸ”´ ĞšĞĞĞŸĞšĞ˜ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ˜ Ğ˜ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ˜ ---\n",
        "    with gr.Row():\n",
        "        generate_btn = gr.Button(\"ğŸ¤ Generate Speech\", variant=\"primary\", size=\"lg\")\n",
        "        stop_btn = gr.Button(\"ğŸ›‘ Stop Generation\", variant=\"stop\", size=\"lg\")\n",
        "\n",
        "    with gr.Row():\n",
        "        audio_output = gr.Audio(label=\"ğŸ§ ĞÑƒĞ´Ğ¸Ğ¾\", type=\"filepath\")\n",
        "        spectrogram_output = gr.Image(label=\"Spectrogram\", type=\"filepath\")\n",
        "\n",
        "    # --- ğŸ”´ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ ---\n",
        "    stop_btn.click(\n",
        "        fn=stop_generation_process,\n",
        "        outputs=None,\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "    # ğŸ”´ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ™ Ğ’Ğ«Ğ—ĞĞ’ Ğ¡Ğ˜ĞĞ¢Ğ•Ğ—Ğ Ğ¡ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ•Ğœ Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡Ğ\n",
        "    generate_btn.click(\n",
        "        synthesize,\n",
        "        inputs=[\n",
        "            ref_audio_input,\n",
        "            ref_text_input,\n",
        "            gen_text_input,\n",
        "            remove_silence,\n",
        "            seed_input,\n",
        "            remember_seed_checkbox,\n",
        "            model_choice,\n",
        "            cross_fade_slider,\n",
        "            nfe_slider,\n",
        "            speed_slider,\n",
        "            sway_sampling_slider,\n",
        "            cfg_strength_slider,\n",
        "            audio_format,\n",
        "            bitrate,\n",
        "        ],\n",
        "        outputs=[\n",
        "            audio_output, \n",
        "            spectrogram_output, \n",
        "            ref_text_input, \n",
        "            gen_text_input, \n",
        "            seed_input,\n",
        "            model_status  # ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ¯Ğ•Ğœ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ• Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡Ğ ĞœĞĞ”Ğ•Ğ›Ğ˜\n",
        "        ]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ğŸ”´ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸš€ ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ Ğš Ğ—ĞĞŸĞ£Ğ¡ĞšĞ£\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    models_ok = check_all_models_before_launch()\n",
        "    \n",
        "    if models_ok:\n",
        "        print(\"\\nâœ… Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ñ‹, Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ...\")\n",
        "        app.launch(share=True)\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ ĞĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾\")\n",
        "        print(\"   Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ±ÑƒĞ´ĞµÑ‚ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ\")\n",
        "        app.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
